{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import EstimatorPreprocessor as ep\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ep.load_cleaned_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lenc = LabelEncoder()\n",
    "y = lenc.fit_transform(data[\"sentiment\"])\n",
    "\n",
    "# Vectorize text using tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(preprocessor=' '.join, lowercase=False, min_df=5) # min_df = Minimum occurance of words\n",
    "X = tfidf.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# for sentence in data[\"text\"][indices][:30]:\n",
    "#     vs = analyzer.polarity_scores(\" \".join(sentence))\n",
    "#     print(\"{:} {}\".format(\" \".join(sentence), str(vs)))\n",
    "#     print(\"-\"*40)\n",
    "\n",
    "\n",
    "def vader_sentiment_labels(query_text, compound_score):\n",
    "    sentiment_list = []\n",
    "    for item in query_text:\n",
    "        sentence = tfidf.inverse_transform(item)\n",
    "        vs = analyzer.polarity_scores(\" \".join(sentence[0]))\n",
    "        if vs[\"compound\"] > compound_score:\n",
    "            sentiment_list.append(np.int(2)) # Positive Sentiment\n",
    "        elif vs[\"compound\"] < -compound_score:\n",
    "            sentiment_list.append(np.int(0)) # Negative Sentiment\n",
    "        else:\n",
    "            sentiment_list.append(np.int(1)) # Neutral Sentiment\n",
    "    return sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_scores = [0.10] #[0.03, 0.05, 0.07, 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "s = \"\"\n",
    "for score in compound_scores:\n",
    "    y_pred = vader_sentiment_labels(X, score)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    s = \"Score: \" + str(s) + \";Accuracy:\" + str(acc)\n",
    "    history.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38837276656418485"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vader_sentiment_labels(X, 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.50      0.38     24088\n",
      "           1       0.64      0.40      0.49    121113\n",
      "           2       0.14      0.27      0.19     34343\n",
      "\n",
      "    accuracy                           0.39    179544\n",
      "   macro avg       0.36      0.39      0.35    179544\n",
      "weighted avg       0.50      0.39      0.42    179544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use best params of model for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a199850fbe9b2f1658a16eea735881451fc009eba53b7dda86327ce82228d5dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.thesis': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
