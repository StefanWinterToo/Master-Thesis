{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, CuDNNLSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import EstimatorPreprocessor as ep\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ep.load_cleaned_submissions()\n",
    "y = ep.encode_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "sentences = data[\"text\"].tolist()\n",
    "max_sentence_len = ep.max_sentence_length(sentences, truncate=True, max_len = 90)\n",
    "for sentence in sentences:\n",
    "    new_sentences.append(sentence[:max_sentence_len])\n",
    "sentences = new_sentences\n",
    "del new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can maybe be removed\n",
    "# Todo: Loop over hyperparameters\n",
    "vec_size = [50, 100, 200]\n",
    "min_c = [1]\n",
    "w = [1, 2, 3]\n",
    "\n",
    "for vec in vec_size:\n",
    "    for mc in min_c:\n",
    "        for win in w:\n",
    "            print(vec, mc, win)\n",
    "            word_model, pretrained_weights, vocab_size, embedding_size = ep.embedding_word2vec(sentences, vec_size = vec, min_c = mc, w = win)\n",
    "            for word in ['moon', 'short', 'robinhood', 'andromeda', 'ape', 'ðŸ¦']:\n",
    "                most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                                        for similar, dist in word_model.wv.most_similar(word)[:3])\n",
    "                print('  %s -> %s' % (word, most_similar))\n",
    "            print(\"-----------------\"*5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate model with best hyperparameters\n",
    "word_model, pretrained_weights, vocab_size, embedding_size = ep.embedding_word2vec(sentences, vec_size = 50, min_c = 1, w = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  robinhood -> rh (0.98), etrade (0.87), webull (0.86)\n",
      "  andromeda -> jupiter (0.95), mars (0.94), uranus (0.93)\n",
      "  ape -> autist (0.94), monkey (0.91), retard (0.90)\n",
      "  hedgefund -> hfs (0.93), hf (0.89), shorter (0.88)\n",
      "  ðŸ¦ -> ðŸŒ (0.91), ðŸ¦§ (0.91), ðŸ’ (0.87)\n"
     ]
    }
   ],
   "source": [
    "for word in ['robinhood', 'andromeda', 'ape', 'hedgefund', 'ðŸ¦']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                           for similar, dist in word_model.wv.most_similar(word)[:3])\n",
    "  print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "y_train_lstm = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence):\n",
    "    X_train_lstm[i, t] = ep.word2idx(word_model, word)\n",
    "  #y_train_lstm[i] = word2idx(sentence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm, X_test_lstm, y_train, y_test = train_test_split(X_train_lstm, y_train, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(d = 0.25, opt = \"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_size, weights = [pretrained_weights]))\n",
    "    model.add(CuDNNLSTM(units = embedding_size))\n",
    "    model.add(Dropout(d))\n",
    "    model.add(Dense(3, activation = \"softmax\"))\n",
    "    model.compile(opt, \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1684/1684 - 71s - loss: 0.7803 - acc: 0.6931 - val_loss: 0.5248 - val_acc: 0.7963 - 71s/epoch - 42ms/step\n",
      "Epoch 2/3\n",
      "1684/1684 - 17s - loss: 0.3874 - acc: 0.8580 - val_loss: 0.3302 - val_acc: 0.8799 - 17s/epoch - 10ms/step\n",
      "Epoch 3/3\n",
      "1684/1684 - 17s - loss: 0.2574 - acc: 0.9126 - val_loss: 0.3153 - val_acc: 0.8859 - 17s/epoch - 10ms/step\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "val_acc = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "param_list = []\n",
    "\n",
    "dropout = [0, 0.25, 0.5]\n",
    "optimi = [\"rmsprop\", \"SGD\", \"Adam\"]\n",
    "\n",
    "# Best model -> also change epochs\n",
    "dropout = [0]\n",
    "optimi = [\"Adam\"]\n",
    "\n",
    "for d in dropout:\n",
    "    for opt in optimi:\n",
    "        model = build_model(d)\n",
    "        history = model.fit(X_train_lstm, y_train, epochs = 3, validation_split = 0.2, batch_size = 64, verbose = 2)\n",
    "        # acc.append(history.history['acc'])\n",
    "        # val_acc.append(history.history['val_acc'])\n",
    "        # loss.append(history.history['loss'])\n",
    "        # val_loss.append(history.history['val_loss'])\n",
    "        param_list.append(\"Dropout: \" + str(d) + \n",
    "                            \" & Optimizer:\" + opt +\n",
    "                            \" ;Train Acc: \" + str(history.history['acc']) + \n",
    "                            \" ;Train Loss: \"+ str(history.history['loss']) + \n",
    "                            \" ;Val Acc:\" + str(history.history['val_acc']) +\n",
    "                            \" ;Val Loss:\" + str(history.history['val_loss'])\n",
    "                            )\n",
    "        history.model.save(\"./model/lstm/lstm\" + str(d) + \"_\" + opt + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"./eval_model/lstm_final.txt\", \"w\")\n",
    "e = \" ;Epoch: \" + str(list(np.arange(1, len(history.history[\"acc\"])+1, 1)))\n",
    "for item in param_list:\n",
    "    textfile.write(item + e + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all parameters to text file\n",
    "ep.save_param_list_keras(path = \"./eval_model/lstm_final.txt\", p_list = param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model with best metrics and given hyperparameters then\n",
    "# print classification report\n",
    "loaded_model = load_model('./model/lstm/lstm0_Adam.h5') # First retrain model and then load the appropriate one\n",
    "test_pred = loaded_model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_test_pred = []\n",
    "for i in range(len(test_pred)):\n",
    "    maxi = np.argmax(test_pred[i])\n",
    "    arr = np.zeros((3))\n",
    "    arr[maxi] = 1\n",
    "    converted_test_pred.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      6022\n",
      "           1       0.92      0.93      0.93     30278\n",
      "           2       0.78      0.80      0.79      8586\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     44886\n",
      "   macro avg       0.85      0.84      0.84     44886\n",
      "weighted avg       0.89      0.89      0.89     44886\n",
      " samples avg       0.89      0.89      0.89     44886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, converted_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8859778104531479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, converted_test_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cbf719e3a6bae849a7ceaf0338c4b24a4d1a8685d6b4521732eb78799be0b2d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.thesis': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
