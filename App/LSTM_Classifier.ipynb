{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, CuDNNLSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import EstimatorPreprocessor as ep\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ep.load_cleaned_submissions()\n",
    "y = ep.encode_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "sentences = data[\"text\"].tolist()\n",
    "max_sentence_len = ep.max_sentence_length(sentences, truncate=True, max_len = 90)\n",
    "for sentence in sentences:\n",
    "    new_sentences.append(sentence[:max_sentence_len])\n",
    "sentences = new_sentences\n",
    "del new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can maybe be removed\n",
    "# Todo: Loop over hyperparameters\n",
    "vec_size = [50, 100, 200]\n",
    "min_c = [1]\n",
    "w = [1, 2, 3]\n",
    "\n",
    "for vec in vec_size:\n",
    "    for mc in min_c:\n",
    "        for win in w:\n",
    "            print(vec, mc, win)\n",
    "            word_model, pretrained_weights, vocab_size, embedding_size = ep.embedding_word2vec(sentences, vec_size = vec, min_c = mc, w = win)\n",
    "            for word in ['moon', 'short', 'robinhood', 'andromeda', 'ape', '🦍']:\n",
    "                most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                                        for similar, dist in word_model.wv.most_similar(word)[:3])\n",
    "                print('  %s -> %s' % (word, most_similar))\n",
    "            print(\"-----------------\"*5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate model with best hyperparameters\n",
    "word_model, pretrained_weights, vocab_size, embedding_size = ep.embedding_word2vec(sentences, vec_size = 50, min_c = 1, w = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  robinhood -> rh (0.98), etrade (0.87), webull (0.86)\n",
      "  andromeda -> jupiter (0.95), mars (0.94), uranus (0.93)\n",
      "  ape -> autist (0.94), monkey (0.91), retard (0.90)\n",
      "  hedgefund -> hfs (0.93), hf (0.89), shorter (0.88)\n",
      "  🦍 -> 🍌 (0.91), 🦧 (0.91), 🐒 (0.87)\n"
     ]
    }
   ],
   "source": [
    "for word in ['robinhood', 'andromeda', 'ape', 'hedgefund', '🦍']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                           for similar, dist in word_model.wv.most_similar(word)[:3])\n",
    "  print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "y_train_lstm = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence):\n",
    "    X_train_lstm[i, t] = ep.word2idx(word_model, word)\n",
    "  #y_train_lstm[i] = word2idx(sentence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm, X_test_lstm, y_train, y_test = train_test_split(X_train_lstm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(d = 0.25, opt = \"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_size, weights = [pretrained_weights]))\n",
    "    model.add(CuDNNLSTM(units = embedding_size))\n",
    "    model.add(Dropout(d))\n",
    "    model.add(Dense(3, activation = \"softmax\"))\n",
    "    model.compile(opt, \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1684/1684 - 22s - loss: 0.6322 - acc: 0.7470 - val_loss: 0.4700 - val_acc: 0.8251 - 22s/epoch - 13ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 17s - loss: 0.3877 - acc: 0.8581 - val_loss: 0.4147 - val_acc: 0.8450 - 17s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 17s - loss: 0.2583 - acc: 0.9113 - val_loss: 0.3428 - val_acc: 0.8774 - 17s/epoch - 10ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 17s - loss: 0.1923 - acc: 0.9355 - val_loss: 0.3424 - val_acc: 0.8846 - 17s/epoch - 10ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 17s - loss: 0.1612 - acc: 0.9477 - val_loss: 0.4352 - val_acc: 0.8589 - 17s/epoch - 10ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 17s - loss: 0.1433 - acc: 0.9542 - val_loss: 0.4142 - val_acc: 0.8764 - 17s/epoch - 10ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 17s - loss: 0.1297 - acc: 0.9585 - val_loss: 0.4397 - val_acc: 0.8730 - 17s/epoch - 10ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 17s - loss: 0.1213 - acc: 0.9617 - val_loss: 0.4761 - val_acc: 0.8719 - 17s/epoch - 10ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 17s - loss: 0.1129 - acc: 0.9645 - val_loss: 0.5298 - val_acc: 0.8640 - 17s/epoch - 10ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1059 - acc: 0.9671 - val_loss: 0.5509 - val_acc: 0.8674 - 16s/epoch - 10ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.0997 - acc: 0.9697 - val_loss: 0.5569 - val_acc: 0.8660 - 16s/epoch - 10ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.0946 - acc: 0.9710 - val_loss: 0.6006 - val_acc: 0.8545 - 16s/epoch - 10ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.0885 - acc: 0.9737 - val_loss: 0.6261 - val_acc: 0.8614 - 16s/epoch - 10ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.0846 - acc: 0.9745 - val_loss: 0.6362 - val_acc: 0.8580 - 16s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0794 - acc: 0.9766 - val_loss: 0.6947 - val_acc: 0.8560 - 16s/epoch - 10ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0752 - acc: 0.9779 - val_loss: 0.7697 - val_acc: 0.8456 - 16s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0710 - acc: 0.9796 - val_loss: 0.7354 - val_acc: 0.8485 - 16s/epoch - 10ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 17s - loss: 0.0675 - acc: 0.9810 - val_loss: 0.7400 - val_acc: 0.8543 - 17s/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0643 - acc: 0.9818 - val_loss: 0.8036 - val_acc: 0.8472 - 16s/epoch - 10ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0604 - acc: 0.9831 - val_loss: 0.7921 - val_acc: 0.8502 - 16s/epoch - 10ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8335 - acc: 0.6785 - val_loss: 0.7897 - val_acc: 0.6992 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.5953 - acc: 0.7640 - val_loss: 0.3992 - val_acc: 0.8507 - 16s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.3193 - acc: 0.8871 - val_loss: 0.3349 - val_acc: 0.8842 - 16s/epoch - 9ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.2248 - acc: 0.9251 - val_loss: 0.3557 - val_acc: 0.8785 - 16s/epoch - 9ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.1761 - acc: 0.9418 - val_loss: 0.3705 - val_acc: 0.8788 - 16s/epoch - 9ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1534 - acc: 0.9504 - val_loss: 0.3942 - val_acc: 0.8872 - 16s/epoch - 9ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1398 - acc: 0.9551 - val_loss: 0.4499 - val_acc: 0.8697 - 16s/epoch - 9ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1297 - acc: 0.9591 - val_loss: 0.4643 - val_acc: 0.8801 - 16s/epoch - 9ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1213 - acc: 0.9622 - val_loss: 0.5083 - val_acc: 0.8700 - 16s/epoch - 9ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1140 - acc: 0.9644 - val_loss: 0.5389 - val_acc: 0.8673 - 16s/epoch - 9ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1071 - acc: 0.9677 - val_loss: 0.5434 - val_acc: 0.8686 - 16s/epoch - 9ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.1006 - acc: 0.9696 - val_loss: 0.5917 - val_acc: 0.8676 - 16s/epoch - 9ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.0962 - acc: 0.9713 - val_loss: 0.6092 - val_acc: 0.8628 - 16s/epoch - 9ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.0904 - acc: 0.9730 - val_loss: 0.6263 - val_acc: 0.8667 - 16s/epoch - 9ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0854 - acc: 0.9746 - val_loss: 0.6403 - val_acc: 0.8583 - 16s/epoch - 9ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0811 - acc: 0.9762 - val_loss: 0.6872 - val_acc: 0.8565 - 16s/epoch - 9ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0775 - acc: 0.9772 - val_loss: 0.6940 - val_acc: 0.8550 - 16s/epoch - 9ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0735 - acc: 0.9783 - val_loss: 0.6957 - val_acc: 0.8582 - 16s/epoch - 9ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0698 - acc: 0.9798 - val_loss: 0.7448 - val_acc: 0.8542 - 16s/epoch - 9ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0671 - acc: 0.9808 - val_loss: 0.7601 - val_acc: 0.8553 - 16s/epoch - 9ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8157 - acc: 0.6824 - val_loss: 0.6019 - val_acc: 0.7512 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.4054 - acc: 0.8475 - val_loss: 0.3330 - val_acc: 0.8812 - 16s/epoch - 9ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.2519 - acc: 0.9123 - val_loss: 0.3136 - val_acc: 0.8903 - 16s/epoch - 9ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.1887 - acc: 0.9373 - val_loss: 0.3539 - val_acc: 0.8806 - 16s/epoch - 9ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.1588 - acc: 0.9477 - val_loss: 0.3763 - val_acc: 0.8840 - 16s/epoch - 9ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1414 - acc: 0.9540 - val_loss: 0.4301 - val_acc: 0.8727 - 16s/epoch - 9ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1307 - acc: 0.9582 - val_loss: 0.4477 - val_acc: 0.8851 - 16s/epoch - 9ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1212 - acc: 0.9614 - val_loss: 0.4716 - val_acc: 0.8717 - 16s/epoch - 9ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1137 - acc: 0.9642 - val_loss: 0.4900 - val_acc: 0.8728 - 16s/epoch - 9ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1065 - acc: 0.9667 - val_loss: 0.4919 - val_acc: 0.8731 - 16s/epoch - 9ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1005 - acc: 0.9689 - val_loss: 0.5847 - val_acc: 0.8655 - 16s/epoch - 9ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.0949 - acc: 0.9709 - val_loss: 0.5982 - val_acc: 0.8657 - 16s/epoch - 9ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.0899 - acc: 0.9725 - val_loss: 0.5908 - val_acc: 0.8642 - 16s/epoch - 9ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.0851 - acc: 0.9746 - val_loss: 0.6018 - val_acc: 0.8615 - 16s/epoch - 9ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0813 - acc: 0.9758 - val_loss: 0.6421 - val_acc: 0.8540 - 16s/epoch - 9ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0768 - acc: 0.9774 - val_loss: 0.6936 - val_acc: 0.8527 - 16s/epoch - 9ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0729 - acc: 0.9785 - val_loss: 0.7031 - val_acc: 0.8514 - 16s/epoch - 9ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0691 - acc: 0.9799 - val_loss: 0.7599 - val_acc: 0.8496 - 16s/epoch - 9ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0673 - acc: 0.9802 - val_loss: 0.7341 - val_acc: 0.8572 - 16s/epoch - 9ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0634 - acc: 0.9818 - val_loss: 0.7609 - val_acc: 0.8579 - 16s/epoch - 9ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8535 - acc: 0.6748 - val_loss: 0.8550 - val_acc: 0.6711 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.5598 - acc: 0.7751 - val_loss: 0.3904 - val_acc: 0.8535 - 16s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.3180 - acc: 0.8879 - val_loss: 0.3283 - val_acc: 0.8810 - 16s/epoch - 10ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.2328 - acc: 0.9217 - val_loss: 0.3332 - val_acc: 0.8860 - 16s/epoch - 10ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.1866 - acc: 0.9385 - val_loss: 0.3369 - val_acc: 0.8882 - 16s/epoch - 10ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1605 - acc: 0.9478 - val_loss: 0.3779 - val_acc: 0.8805 - 16s/epoch - 10ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1453 - acc: 0.9532 - val_loss: 0.4092 - val_acc: 0.8832 - 16s/epoch - 10ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1329 - acc: 0.9572 - val_loss: 0.4353 - val_acc: 0.8784 - 16s/epoch - 10ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1237 - acc: 0.9610 - val_loss: 0.4977 - val_acc: 0.8770 - 16s/epoch - 10ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1160 - acc: 0.9637 - val_loss: 0.5200 - val_acc: 0.8782 - 16s/epoch - 10ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1089 - acc: 0.9666 - val_loss: 0.5456 - val_acc: 0.8673 - 16s/epoch - 10ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.1029 - acc: 0.9684 - val_loss: 0.6208 - val_acc: 0.8692 - 16s/epoch - 10ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.0973 - acc: 0.9711 - val_loss: 0.5998 - val_acc: 0.8679 - 16s/epoch - 10ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.0922 - acc: 0.9725 - val_loss: 0.6285 - val_acc: 0.8709 - 16s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0869 - acc: 0.9743 - val_loss: 0.6485 - val_acc: 0.8689 - 16s/epoch - 10ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0836 - acc: 0.9757 - val_loss: 0.6991 - val_acc: 0.8612 - 16s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0806 - acc: 0.9769 - val_loss: 0.7390 - val_acc: 0.8626 - 16s/epoch - 10ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0747 - acc: 0.9787 - val_loss: 0.7540 - val_acc: 0.8625 - 16s/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0730 - acc: 0.9795 - val_loss: 0.7440 - val_acc: 0.8628 - 16s/epoch - 10ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0683 - acc: 0.9811 - val_loss: 0.7487 - val_acc: 0.8656 - 16s/epoch - 10ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8233 - acc: 0.6856 - val_loss: 0.6313 - val_acc: 0.7352 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.5130 - acc: 0.7953 - val_loss: 0.4192 - val_acc: 0.8438 - 16s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 17s - loss: 0.3239 - acc: 0.8868 - val_loss: 0.3412 - val_acc: 0.8774 - 17s/epoch - 10ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.2366 - acc: 0.9220 - val_loss: 0.3361 - val_acc: 0.8886 - 16s/epoch - 10ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.1906 - acc: 0.9386 - val_loss: 0.3625 - val_acc: 0.8875 - 16s/epoch - 10ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1652 - acc: 0.9471 - val_loss: 0.4122 - val_acc: 0.8755 - 16s/epoch - 10ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1494 - acc: 0.9528 - val_loss: 0.3825 - val_acc: 0.8829 - 16s/epoch - 10ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1381 - acc: 0.9570 - val_loss: 0.4502 - val_acc: 0.8696 - 16s/epoch - 10ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1280 - acc: 0.9603 - val_loss: 0.4965 - val_acc: 0.8752 - 16s/epoch - 10ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1208 - acc: 0.9630 - val_loss: 0.5198 - val_acc: 0.8726 - 16s/epoch - 10ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1146 - acc: 0.9650 - val_loss: 0.5198 - val_acc: 0.8676 - 16s/epoch - 10ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.1093 - acc: 0.9668 - val_loss: 0.5489 - val_acc: 0.8726 - 16s/epoch - 10ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.1038 - acc: 0.9690 - val_loss: 0.5576 - val_acc: 0.8679 - 16s/epoch - 10ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.0971 - acc: 0.9715 - val_loss: 0.6268 - val_acc: 0.8671 - 16s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0937 - acc: 0.9725 - val_loss: 0.6363 - val_acc: 0.8676 - 16s/epoch - 10ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0884 - acc: 0.9745 - val_loss: 0.6652 - val_acc: 0.8651 - 16s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0849 - acc: 0.9757 - val_loss: 0.6699 - val_acc: 0.8642 - 16s/epoch - 10ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0809 - acc: 0.9771 - val_loss: 0.6910 - val_acc: 0.8600 - 16s/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0779 - acc: 0.9778 - val_loss: 0.6984 - val_acc: 0.8618 - 16s/epoch - 10ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0750 - acc: 0.9792 - val_loss: 0.7347 - val_acc: 0.8613 - 16s/epoch - 10ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8168 - acc: 0.6861 - val_loss: 0.6102 - val_acc: 0.7468 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.4891 - acc: 0.8022 - val_loss: 0.4043 - val_acc: 0.8500 - 16s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.3212 - acc: 0.8868 - val_loss: 0.3528 - val_acc: 0.8716 - 16s/epoch - 10ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.2330 - acc: 0.9214 - val_loss: 0.3324 - val_acc: 0.8846 - 16s/epoch - 10ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.1887 - acc: 0.9381 - val_loss: 0.3537 - val_acc: 0.8834 - 16s/epoch - 10ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1639 - acc: 0.9472 - val_loss: 0.3805 - val_acc: 0.8792 - 16s/epoch - 10ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1490 - acc: 0.9524 - val_loss: 0.3986 - val_acc: 0.8775 - 16s/epoch - 10ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1378 - acc: 0.9562 - val_loss: 0.4268 - val_acc: 0.8736 - 16s/epoch - 10ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1274 - acc: 0.9596 - val_loss: 0.4711 - val_acc: 0.8746 - 16s/epoch - 10ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1201 - acc: 0.9627 - val_loss: 0.4966 - val_acc: 0.8743 - 16s/epoch - 10ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1129 - acc: 0.9655 - val_loss: 0.5493 - val_acc: 0.8694 - 16s/epoch - 10ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.1067 - acc: 0.9679 - val_loss: 0.5596 - val_acc: 0.8606 - 16s/epoch - 10ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.1002 - acc: 0.9700 - val_loss: 0.6012 - val_acc: 0.8578 - 16s/epoch - 10ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.0942 - acc: 0.9724 - val_loss: 0.6383 - val_acc: 0.8611 - 16s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0910 - acc: 0.9731 - val_loss: 0.6287 - val_acc: 0.8683 - 16s/epoch - 10ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0855 - acc: 0.9755 - val_loss: 0.6830 - val_acc: 0.8643 - 16s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0815 - acc: 0.9770 - val_loss: 0.6943 - val_acc: 0.8614 - 16s/epoch - 10ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0776 - acc: 0.9783 - val_loss: 0.6637 - val_acc: 0.8602 - 16s/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0746 - acc: 0.9794 - val_loss: 0.7569 - val_acc: 0.8502 - 16s/epoch - 10ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0710 - acc: 0.9804 - val_loss: 0.7567 - val_acc: 0.8609 - 16s/epoch - 10ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8598 - acc: 0.6740 - val_loss: 0.8544 - val_acc: 0.6711 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.7254 - acc: 0.7100 - val_loss: 0.5238 - val_acc: 0.7632 - 16s/epoch - 9ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.4406 - acc: 0.8319 - val_loss: 0.3908 - val_acc: 0.8554 - 16s/epoch - 9ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.2991 - acc: 0.8987 - val_loss: 0.3527 - val_acc: 0.8715 - 16s/epoch - 9ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.2300 - acc: 0.9254 - val_loss: 0.3647 - val_acc: 0.8838 - 16s/epoch - 9ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1928 - acc: 0.9380 - val_loss: 0.3906 - val_acc: 0.8789 - 16s/epoch - 9ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1695 - acc: 0.9463 - val_loss: 0.4267 - val_acc: 0.8790 - 16s/epoch - 9ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1548 - acc: 0.9509 - val_loss: 0.4468 - val_acc: 0.8797 - 16s/epoch - 9ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1440 - acc: 0.9550 - val_loss: 0.4856 - val_acc: 0.8790 - 16s/epoch - 9ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1344 - acc: 0.9583 - val_loss: 0.5190 - val_acc: 0.8630 - 16s/epoch - 9ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1264 - acc: 0.9611 - val_loss: 0.5784 - val_acc: 0.8744 - 16s/epoch - 9ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.1198 - acc: 0.9632 - val_loss: 0.5733 - val_acc: 0.8745 - 16s/epoch - 9ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.1127 - acc: 0.9658 - val_loss: 0.5580 - val_acc: 0.8732 - 16s/epoch - 9ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.1079 - acc: 0.9678 - val_loss: 0.6347 - val_acc: 0.8704 - 16s/epoch - 9ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.1052 - acc: 0.9688 - val_loss: 0.5697 - val_acc: 0.8689 - 16s/epoch - 9ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0997 - acc: 0.9705 - val_loss: 0.6447 - val_acc: 0.8669 - 16s/epoch - 9ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0948 - acc: 0.9725 - val_loss: 0.6746 - val_acc: 0.8605 - 16s/epoch - 9ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0918 - acc: 0.9738 - val_loss: 0.6565 - val_acc: 0.8645 - 16s/epoch - 9ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0887 - acc: 0.9745 - val_loss: 0.7437 - val_acc: 0.8561 - 16s/epoch - 9ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0849 - acc: 0.9762 - val_loss: 0.7140 - val_acc: 0.8616 - 16s/epoch - 9ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8358 - acc: 0.6798 - val_loss: 0.6671 - val_acc: 0.7282 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.5326 - acc: 0.7821 - val_loss: 0.4792 - val_acc: 0.8243 - 16s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.3738 - acc: 0.8656 - val_loss: 0.3727 - val_acc: 0.8702 - 16s/epoch - 10ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 16s - loss: 0.2667 - acc: 0.9108 - val_loss: 0.3517 - val_acc: 0.8832 - 16s/epoch - 10ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 16s - loss: 0.2145 - acc: 0.9314 - val_loss: 0.3638 - val_acc: 0.8824 - 16s/epoch - 10ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 16s - loss: 0.1846 - acc: 0.9426 - val_loss: 0.3753 - val_acc: 0.8785 - 16s/epoch - 10ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 16s - loss: 0.1671 - acc: 0.9482 - val_loss: 0.3776 - val_acc: 0.8791 - 16s/epoch - 10ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 16s - loss: 0.1525 - acc: 0.9527 - val_loss: 0.4171 - val_acc: 0.8704 - 16s/epoch - 10ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 16s - loss: 0.1407 - acc: 0.9569 - val_loss: 0.4195 - val_acc: 0.8756 - 16s/epoch - 10ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 16s - loss: 0.1326 - acc: 0.9595 - val_loss: 0.4310 - val_acc: 0.8784 - 16s/epoch - 10ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 16s - loss: 0.1243 - acc: 0.9630 - val_loss: 0.4634 - val_acc: 0.8745 - 16s/epoch - 10ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 16s - loss: 0.1177 - acc: 0.9651 - val_loss: 0.4925 - val_acc: 0.8740 - 16s/epoch - 10ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 16s - loss: 0.1108 - acc: 0.9678 - val_loss: 0.5134 - val_acc: 0.8683 - 16s/epoch - 10ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 16s - loss: 0.1045 - acc: 0.9696 - val_loss: 0.5972 - val_acc: 0.8585 - 16s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 16s - loss: 0.0989 - acc: 0.9716 - val_loss: 0.6206 - val_acc: 0.8673 - 16s/epoch - 10ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 16s - loss: 0.0950 - acc: 0.9731 - val_loss: 0.5865 - val_acc: 0.8630 - 16s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 16s - loss: 0.0894 - acc: 0.9751 - val_loss: 0.6673 - val_acc: 0.8616 - 16s/epoch - 10ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 16s - loss: 0.0855 - acc: 0.9760 - val_loss: 0.6470 - val_acc: 0.8656 - 16s/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 16s - loss: 0.0814 - acc: 0.9780 - val_loss: 0.6685 - val_acc: 0.8650 - 16s/epoch - 10ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 16s - loss: 0.0782 - acc: 0.9788 - val_loss: 0.6757 - val_acc: 0.8620 - 16s/epoch - 10ms/step\n",
      "Epoch 1/20\n",
      "1684/1684 - 17s - loss: 0.8185 - acc: 0.6876 - val_loss: 0.6125 - val_acc: 0.7445 - 17s/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "1684/1684 - 16s - loss: 0.5190 - acc: 0.7885 - val_loss: 0.4538 - val_acc: 0.8256 - 16s/epoch - 9ms/step\n",
      "Epoch 3/20\n",
      "1684/1684 - 16s - loss: 0.3556 - acc: 0.8762 - val_loss: 0.3505 - val_acc: 0.8768 - 16s/epoch - 9ms/step\n",
      "Epoch 4/20\n",
      "1684/1684 - 19s - loss: 0.2578 - acc: 0.9158 - val_loss: 0.3538 - val_acc: 0.8804 - 19s/epoch - 11ms/step\n",
      "Epoch 5/20\n",
      "1684/1684 - 17s - loss: 0.2073 - acc: 0.9337 - val_loss: 0.3719 - val_acc: 0.8765 - 17s/epoch - 10ms/step\n",
      "Epoch 6/20\n",
      "1684/1684 - 18s - loss: 0.1800 - acc: 0.9426 - val_loss: 0.3825 - val_acc: 0.8834 - 18s/epoch - 10ms/step\n",
      "Epoch 7/20\n",
      "1684/1684 - 17s - loss: 0.1612 - acc: 0.9493 - val_loss: 0.4185 - val_acc: 0.8802 - 17s/epoch - 10ms/step\n",
      "Epoch 8/20\n",
      "1684/1684 - 17s - loss: 0.1478 - acc: 0.9544 - val_loss: 0.4055 - val_acc: 0.8813 - 17s/epoch - 10ms/step\n",
      "Epoch 9/20\n",
      "1684/1684 - 17s - loss: 0.1382 - acc: 0.9575 - val_loss: 0.4632 - val_acc: 0.8743 - 17s/epoch - 10ms/step\n",
      "Epoch 10/20\n",
      "1684/1684 - 17s - loss: 0.1293 - acc: 0.9601 - val_loss: 0.4693 - val_acc: 0.8782 - 17s/epoch - 10ms/step\n",
      "Epoch 11/20\n",
      "1684/1684 - 17s - loss: 0.1214 - acc: 0.9634 - val_loss: 0.5659 - val_acc: 0.8686 - 17s/epoch - 10ms/step\n",
      "Epoch 12/20\n",
      "1684/1684 - 17s - loss: 0.1143 - acc: 0.9662 - val_loss: 0.5950 - val_acc: 0.8702 - 17s/epoch - 10ms/step\n",
      "Epoch 13/20\n",
      "1684/1684 - 17s - loss: 0.1083 - acc: 0.9682 - val_loss: 0.6166 - val_acc: 0.8721 - 17s/epoch - 10ms/step\n",
      "Epoch 14/20\n",
      "1684/1684 - 17s - loss: 0.1023 - acc: 0.9702 - val_loss: 0.6087 - val_acc: 0.8657 - 17s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1684/1684 - 17s - loss: 0.0969 - acc: 0.9724 - val_loss: 0.6410 - val_acc: 0.8661 - 17s/epoch - 10ms/step\n",
      "Epoch 16/20\n",
      "1684/1684 - 17s - loss: 0.0930 - acc: 0.9739 - val_loss: 0.6903 - val_acc: 0.8670 - 17s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1684/1684 - 17s - loss: 0.0875 - acc: 0.9756 - val_loss: 0.7002 - val_acc: 0.8631 - 17s/epoch - 10ms/step\n",
      "Epoch 18/20\n",
      "1684/1684 - 17s - loss: 0.0847 - acc: 0.9766 - val_loss: 0.7245 - val_acc: 0.8623 - 17s/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "1684/1684 - 17s - loss: 0.0800 - acc: 0.9780 - val_loss: 0.7289 - val_acc: 0.8599 - 17s/epoch - 10ms/step\n",
      "Epoch 20/20\n",
      "1684/1684 - 18s - loss: 0.0776 - acc: 0.9785 - val_loss: 0.7135 - val_acc: 0.8617 - 18s/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "val_acc = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "param_list = []\n",
    "\n",
    "dropout = [0, 0.25, 0.5]\n",
    "optimi = [\"rmsprop\", \"SGD\", \"Adam\"]\n",
    "\n",
    "for d in dropout:\n",
    "    for opt in optimi:\n",
    "        model = build_model(d)\n",
    "        history = model.fit(X_train_lstm, y_train, epochs = 20, validation_split = 0.2, batch_size = 64, verbose = 2)\n",
    "        # acc.append(history.history['acc'])\n",
    "        # val_acc.append(history.history['val_acc'])\n",
    "        # loss.append(history.history['loss'])\n",
    "        # val_loss.append(history.history['val_loss'])\n",
    "        param_list.append(\"Dropout: \" + str(d) + \n",
    "                            \" & Optimizer:\" + opt +\n",
    "                            \" ;Train Acc: \" + str(history.history['acc']) + \n",
    "                            \" ;Train Loss: \"+ str(history.history['loss']) + \n",
    "                            \" ;Val Acc:\" + str(history.history['val_acc']) +\n",
    "                            \" ;Val Loss:\" + str(history.history['val_loss'])\n",
    "                            )\n",
    "        history.model.save(\"./model/lstm/lstm\" + str(d) + \"_\" + opt + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"./eval_model/lstm_final.txt\", \"w\")\n",
    "e = \" ;Epoch: \" + str(list(np.arange(1, len(history.history[\"acc\"])+1, 1)))\n",
    "for item in param_list:\n",
    "    textfile.write(item + e + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'EstimatorPreprocessor' has no attribute 'save_param_list_keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30628/2208293431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save all parameters to text file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_param_list_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./eval_model/lstm_final.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'EstimatorPreprocessor' has no attribute 'save_param_list_keras'"
     ]
    }
   ],
   "source": [
    "# Save all parameters to text file\n",
    "ep.save_param_list_keras(path = \"./eval_model/lstm_final.txt\", p_list = param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model with best metrics and given hyperparameters then\n",
    "# print classification report\n",
    "loaded_model = load_model('./model/lstm/lstm0.5.h5') # First retrain model and then load the appropriate one\n",
    "test_pred = loaded_model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_test_pred = []\n",
    "for i in range(len(test_pred)):\n",
    "    maxi = np.argmax(test_pred[i])\n",
    "    arr = np.zeros((3))\n",
    "    arr[maxi] = 1\n",
    "    converted_test_pred.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81      6088\n",
      "           1       0.93      0.92      0.92     30156\n",
      "           2       0.77      0.82      0.79      8642\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     44886\n",
      "   macro avg       0.84      0.85      0.84     44886\n",
      "weighted avg       0.88      0.88      0.88     44886\n",
      " samples avg       0.88      0.88      0.88     44886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, converted_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs = 42\n",
    "# Dropout = 0\n",
    "# Optimizer = Adam"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cbf719e3a6bae849a7ceaf0338c4b24a4d1a8685d6b4521732eb78799be0b2d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.thesis': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
