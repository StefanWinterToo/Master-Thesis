{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, CuDNNLSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import EstimatorPreprocessor as ep\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ep.load_cleaned_submissions()\n",
    "y = ep.encode_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "sentences = data[\"text\"].tolist()\n",
    "max_sentence_len = ep.max_sentence_length(sentences, truncate=True, max_len = 90)\n",
    "for sentence in sentences:\n",
    "    new_sentences.append(sentence[:max_sentence_len])\n",
    "sentences = new_sentences\n",
    "del new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can maybe be removed\n",
    "# Todo: Loop over hyperparameters\n",
    "vec_size = [50, 100, 200]\n",
    "min_c = [1]\n",
    "w = [1, 2, 3]\n",
    "\n",
    "for vec in vec_size:\n",
    "    for mc in min_c:\n",
    "        for win in w:\n",
    "            print(vec, mc, win)\n",
    "            word_model, pretrained_weights, vocab_size, embedding_size = ep.embedding_word2vec(sentences, vec_size = vec, min_c = mc, w = win)\n",
    "            for word in ['moon', 'short', 'robinhood', 'andromeda', 'ape', 'ðŸ¦']:\n",
    "                most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                                        for similar, dist in word_model.wv.most_similar(word)[:3])\n",
    "                print('  %s -> %s' % (word, most_similar))\n",
    "            print(\"-----------------\"*5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate model with best hyperparameters\n",
    "word_model, pretrained_weights, vocab_size, embedding_size = ep.embedding_word2vec(sentences, vec_size = 50, min_c = 1, w = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  robinhood -> rh (0.98), etrade (0.87), webull (0.86)\n",
      "  andromeda -> jupiter (0.95), mars (0.94), uranus (0.93)\n",
      "  ape -> autist (0.94), monkey (0.91), retard (0.90)\n",
      "  hedgefund -> hfs (0.93), hf (0.89), shorter (0.88)\n",
      "  ðŸ¦ -> ðŸŒ (0.91), ðŸ¦§ (0.91), ðŸ’ (0.87)\n"
     ]
    }
   ],
   "source": [
    "for word in ['robinhood', 'andromeda', 'ape', 'hedgefund', 'ðŸ¦']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                           for similar, dist in word_model.wv.most_similar(word)[:3])\n",
    "  print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "y_train_lstm = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence):\n",
    "    X_train_lstm[i, t] = ep.word2idx(word_model, word)\n",
    "  #y_train_lstm[i] = word2idx(sentence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm, X_test_lstm, y_train, y_test = train_test_split(X_train_lstm, y_train, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(d = 0.25, opt = \"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_size, weights = [pretrained_weights]))\n",
    "    model.add(CuDNNLSTM(units = embedding_size))\n",
    "    model.add(Dropout(d))\n",
    "    model.add(Dense(3, activation = \"softmax\"))\n",
    "    model.compile(opt, \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1684/1684 - 17s - loss: 0.7477 - acc: 0.7090 - val_loss: 0.4413 - val_acc: 0.8338 - 17s/epoch - 10ms/step\n",
      "Epoch 2/3\n",
      "1684/1684 - 16s - loss: 0.3582 - acc: 0.8680 - val_loss: 0.3202 - val_acc: 0.8878 - 16s/epoch - 9ms/step\n",
      "Epoch 3/3\n",
      "1684/1684 - 15s - loss: 0.2404 - acc: 0.9173 - val_loss: 0.3094 - val_acc: 0.8930 - 15s/epoch - 9ms/step\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "val_acc = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "param_list = []\n",
    "\n",
    "dropout = [0, 0.25, 0.5]\n",
    "optimi = [\"rmsprop\", \"SGD\", \"Adam\"]\n",
    "\n",
    "# Best model -> also change epochs\n",
    "dropout = [0]\n",
    "optimi = [\"Adam\"]\n",
    "\n",
    "for d in dropout:\n",
    "    for opt in optimi:\n",
    "        model = build_model(d)\n",
    "        history = model.fit(X_train_lstm, y_train, epochs = 3, validation_split = 0.2, batch_size = 64, verbose = 2)\n",
    "        # acc.append(history.history['acc'])\n",
    "        # val_acc.append(history.history['val_acc'])\n",
    "        # loss.append(history.history['loss'])\n",
    "        # val_loss.append(history.history['val_loss'])\n",
    "        param_list.append(\"Dropout: \" + str(d) + \n",
    "                            \" & Optimizer:\" + opt +\n",
    "                            \" ;Train Acc: \" + str(history.history['acc']) + \n",
    "                            \" ;Train Loss: \"+ str(history.history['loss']) + \n",
    "                            \" ;Val Acc:\" + str(history.history['val_acc']) +\n",
    "                            \" ;Val Loss:\" + str(history.history['val_loss'])\n",
    "                            )\n",
    "        history.model.save(\"./model/lstm/lstm\" + str(d) + \"_\" + opt + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"./eval_model/lstm_final.txt\", \"w\")\n",
    "e = \" ;Epoch: \" + str(list(np.arange(1, len(history.history[\"acc\"])+1, 1)))\n",
    "for item in param_list:\n",
    "    textfile.write(item + e + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all parameters to text file\n",
    "ep.save_param_list_keras(path = \"./eval_model/lstm_final.txt\", p_list = param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ./model/lstm/lstm0.5.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8220/170051432.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Retrain model with best metrics and given hyperparameters then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# print classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model/lstm/lstm0.5.h5'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# First retrain model and then load the appropriate one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_lstm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No file or directory found at {filepath}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             raise ImportError(\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at ./model/lstm/lstm0.5.h5"
     ]
    }
   ],
   "source": [
    "# Retrain model with best metrics and given hyperparameters then\n",
    "# print classification report\n",
    "loaded_model = load_model('./model/lstm/lstm0.5.h5') # First retrain model and then load the appropriate one\n",
    "test_pred = loaded_model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_test_pred = []\n",
    "for i in range(len(test_pred)):\n",
    "    maxi = np.argmax(test_pred[i])\n",
    "    arr = np.zeros((3))\n",
    "    arr[maxi] = 1\n",
    "    converted_test_pred.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81      6022\n",
      "           1       0.93      0.94      0.93     30278\n",
      "           2       0.82      0.80      0.81      8586\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     44886\n",
      "   macro avg       0.86      0.85      0.85     44886\n",
      "weighted avg       0.89      0.89      0.89     44886\n",
      " samples avg       0.89      0.89      0.89     44886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, converted_test_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cbf719e3a6bae849a7ceaf0338c4b24a4d1a8685d6b4521732eb78799be0b2d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.thesis': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
