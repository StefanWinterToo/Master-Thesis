{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New notebook for BERT until it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle(\"./data/cleaned_submissions.pkl\")\n",
    "data = data.loc[data[\"sentiment\"] != \"\", [\"text\", \"sentiment\"]] # Only get labeled instances\n",
    "\n",
    "# Encode labels\n",
    "lenc = LabelEncoder()\n",
    "y = lenc.fit_transform(data[\"sentiment\"])\n",
    "\n",
    "# Vectorize text using tfidf\n",
    "tfidf = TfidfVectorizer(preprocessor=' '.join, lowercase=False, min_df=5) # min_df = Minimum occurance of words\n",
    "X = tfidf.fit_transform(data[\"text\"])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 17:14:53.065611: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "# DEV!!!\n",
    "# Less data, so I can check if the model works\n",
    "X_train_bert = X_train[:50] # X_train.toarray()[:50, :, None] -> from lstm\n",
    "y_train_bert = y_train[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.sparse import reorder\n",
    "import tensorflow as tf\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, values = coo.data, dense_shape = coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_dictionary(X):\n",
    "    dic = {}\n",
    "    coo = X.tocoo()\n",
    "    for i in range(0, coo.row):\n",
    "        dic[i] = item.col\n",
    "    print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[   0 3057]\n",
      " [   0 2642]\n",
      " [   0 3787]\n",
      " ...\n",
      " [  48 2746]\n",
      " [  48 2137]\n",
      " [  49 2137]], shape=(669, 2), dtype=int64), values=tf.Tensor(\n",
      "[0.59542567 0.60564568 0.51284646 0.1251003  0.62899846 0.74640758\n",
      " 0.21733997 0.15881666 0.15065584 0.14706971 0.12751685 0.14938152\n",
      " 0.12992945 0.14706971 0.16099216 0.16099216 0.12868378 0.11051738\n",
      " 0.12347085 0.12950483 0.10531676 0.12173798 0.10163091 0.13373161\n",
      " 0.52506096 0.14818947 0.09749081 0.10389945 0.08722882 0.09184639\n",
      " 0.31873883 0.08660123 0.09368347 0.06484686 0.11916851 0.07623556\n",
      " 0.09338411 0.22905329 0.09690672 0.09801749 0.04434515 0.08364269\n",
      " 0.07285628 0.10389945 0.08684972 0.10210092 0.07692903 0.09493194\n",
      " 0.07525298 0.11471555 0.07061359 0.11990253 0.06940748 0.07790467\n",
      " 0.08529129 0.06971818 0.0898785  0.05564887 0.48529515 0.41759496\n",
      " 0.36845519 0.29808807 0.37597191 0.40961901 0.22544147 0.07430258\n",
      " 0.65422268 0.48853088 0.56989251 0.09248129 0.41630068 0.37163055\n",
      " 0.81454622 0.15842644 0.39624896 0.36973403 0.41493431 0.36694022\n",
      " 0.36430615 0.3073115  0.33925432 0.2345003  0.04716659 0.49111555\n",
      " 0.41870666 0.58056956 0.37884223 0.10318451 0.30374456 0.38206347\n",
      " 0.4118136  0.18346733 0.19171228 0.20835214 0.17413465 0.31995382\n",
      " 0.30787297 0.25315198 0.2586806  0.21935781 0.23127267 0.11163837\n",
      " 0.20364813 0.11209788 0.11551461 0.14275723 0.05502349 0.08529108\n",
      " 0.40777244 0.33494974 0.29327494 0.26940958 0.24587218 0.27960979\n",
      " 0.12392305 0.22276817 0.34911777 0.25061809 0.23747132 0.14208743\n",
      " 0.30955306 0.04715987 0.44439057 0.40572993 0.46672354 0.48608349\n",
      " 0.37496667 0.19118033 0.08150035 0.42802974 0.35451936 0.26480882\n",
      " 0.26580682 0.29686099 0.25464167 0.12099807 0.19879221 0.22035917\n",
      " 0.25578526 0.2416184  0.15650371 0.19267289 0.24523823 0.05158159\n",
      " 0.21545176 0.41143581 0.44575098 0.32556209 0.26566746 0.29124692\n",
      " 0.26012682 0.34825121 0.26271728 0.2305981  0.05066959 0.23858724\n",
      " 0.3525344  0.40010415 0.35129223 0.28442737 0.15032869 0.33676921\n",
      " 0.30481665 0.22806959 0.32010012 0.21748483 0.21827529 0.17236355\n",
      " 0.05720874 0.430089   0.56210224 0.687223   0.16367369 0.30426914\n",
      " 0.29303158 0.31580738 0.2775444  0.23148528 0.25014114 0.18571831\n",
      " 0.23880513 0.23569762 0.2211726  0.16908166 0.19080718 0.31580738\n",
      " 0.14783759 0.15368726 0.08698886 0.19095124 0.20749295 0.03708343\n",
      " 0.1440861  0.11496494 0.50483904 0.34889896 0.3870641  0.34646927\n",
      " 0.39088797 0.31137375 0.31100265 0.08412977 0.56839403 0.23344323\n",
      " 0.44888351 0.18620904 0.12150347 0.20478258 0.17735255 0.15921707\n",
      " 0.21165962 0.18686668 0.15452189 0.15550755 0.07710065 0.16516581\n",
      " 0.11287836 0.15240891 0.1821386  0.03286808 0.20379323 0.14946969\n",
      " 0.14483503 0.17024171 0.13132271 0.11179779 0.15422142 0.11781896\n",
      " 0.26781003 0.13485059 0.14325849 0.13534172 0.11723714 0.10146401\n",
      " 0.11068527 0.1198988  0.12767539 0.10625132 0.16781691 0.12578306\n",
      " 0.10900636 0.08823434 0.10432341 0.14655508 0.11473629 0.10746446\n",
      " 0.10170224 0.09083574 0.2017845  0.10310918 0.2032612  0.07592798\n",
      " 0.09069447 0.07442118 0.27236798 0.22374171 0.08224691 0.08991324\n",
      " 0.07275123 0.10154712 0.08929327 0.07627491 0.05871526 0.07683008\n",
      " 0.07383516 0.0722848  0.11267737 0.09792026 0.07105014 0.08209744\n",
      " 0.29152384 0.13172934 0.07605733 0.21071352 0.01935179 0.11393183\n",
      " 0.07519054 0.08746744 0.26175276 0.26175276 0.21912491 0.19842069\n",
      " 0.23712098 0.18207386 0.20966456 0.16505588 0.19579991 0.14901198\n",
      " 0.15923801 0.13783249 0.1771934  0.28826486 0.17324552 0.16741182\n",
      " 0.17564945 0.20966456 0.18335191 0.10206409 0.23927538 0.13944601\n",
      " 0.09823959 0.19787729 0.14430244 0.09027683 0.11812901 0.22228096\n",
      " 0.02975407 0.62165863 0.44034148 0.49841066 0.12703007 0.3938148\n",
      " 0.565254   0.54533001 0.61050815 0.10189644 0.93461038 0.35567321\n",
      " 0.59718043 0.41862408 0.22519214 0.36422713 0.38391398 0.36058607\n",
      " 0.08569861 0.28573445 0.71504874 0.62627767 0.12180886 0.8668584\n",
      " 0.45862    0.19551012 0.54789765 0.20850669 0.81014389 0.71601815\n",
      " 0.37115763 0.47075283 0.34566018 0.09203715 0.58546831 0.28214832\n",
      " 0.66772917 0.34673885 0.10737373 0.44136482 0.38486141 0.3508839\n",
      " 0.3799554  0.19965758 0.13183546 0.31626561 0.31439203 0.26157098\n",
      " 0.2500933  0.050171   0.40091408 0.47209412 0.33320705 0.2163728\n",
      " 0.21865484 0.18255175 0.27908833 0.31675687 0.23979636 0.26132937\n",
      " 0.06307773 0.26347012 0.73676773 0.66095083 0.14253879 0.44161454\n",
      " 0.32815918 0.34251488 0.24918494 0.27930639 0.37425151 0.11980691\n",
      " 0.22880119 0.240112   0.19512236 0.26606586 0.15387963 0.20073272\n",
      " 0.05107379 0.34327366 0.30907307 0.20770835 0.64184274 0.19427862\n",
      " 0.12309899 0.22023725 0.10989016 0.4646071  0.04684627 0.20815752\n",
      " 0.24512786 0.182034   0.36740559 0.46203135 0.18697369 0.25156823\n",
      " 0.15109261 0.19293699 0.1480406  0.14059913 0.16658578 0.06929425\n",
      " 0.27450233 0.11525426 0.12838664 0.23286494 0.12429356 0.12051643\n",
      " 0.10721697 0.0295402  0.08695758 0.09157964 0.25459102 0.20232868\n",
      " 0.1841262  0.20483159 0.1989717  0.18024918 0.20232868 0.15822021\n",
      " 0.16943033 0.17554726 0.17874816 0.22580091 0.21614422 0.16504338\n",
      " 0.14138815 0.17470007 0.22594262 0.15777884 0.15866912 0.14012923\n",
      " 0.09230568 0.11681494 0.1128524  0.10268355 0.10965409 0.13600144\n",
      " 0.08097948 0.16419161 0.1107626  0.139686   0.12126468 0.1223208\n",
      " 0.02687769 0.08332541 0.41345427 0.08904436 0.97963447 0.20078922\n",
      " 0.13925517 0.11784759 0.14258276 0.12436517 0.10403598 0.11743813\n",
      " 0.11112402 0.12776115 0.11703853 0.09731241 0.10880643 0.11664834\n",
      " 0.09263233 0.07358986 0.08410735 0.08300669 0.07038552 0.07738378\n",
      " 0.12853256 0.10656215 0.11448235 0.08077975 0.14729488 0.08331378\n",
      " 0.21115655 0.06844293 0.09475623 0.08575842 0.13532196 0.08383858\n",
      " 0.08183223 0.08443572 0.06659078 0.06734246 0.07033821 0.07962456\n",
      " 0.11207364 0.14267225 0.08884171 0.06957646 0.16746457 0.09192281\n",
      " 0.20745974 0.05149479 0.52809891 0.27326793 0.13476389 0.06339562\n",
      " 0.1343319  0.18992702 0.06231279 0.07200156 0.08486006 0.04996056\n",
      " 0.06594403 0.07327671 0.05261612 0.74811708 0.37478037 0.32803766\n",
      " 0.4249265  0.10811695 0.57169813 0.41339087 0.42375201 0.35225697\n",
      " 0.26926626 0.08937147 0.34369596 0.2885145  0.57032182 0.65648569\n",
      " 0.12299399 0.38130227 0.88347136 0.4498044  0.13097466 0.12786974\n",
      " 0.13009705 0.13009705 0.12786974 0.11068152 0.11344206 0.12594035\n",
      " 0.12008181 0.11270093 0.10381215 0.12594035 0.20396145 0.09341158\n",
      " 0.10342682 0.09049483 0.11006657 0.0858794  0.14883075 0.0785891\n",
      " 0.08265521 0.0901874  0.1612716  0.23785056 0.10098385 0.07626475\n",
      " 0.07493518 0.07661718 0.06840447 0.41282594 0.07569582 0.11270093\n",
      " 0.08717773 0.07609171 0.06246756 0.32086095 0.08212329 0.07300579\n",
      " 0.0703483  0.25283475 0.05072817 0.08339635 0.05963727 0.17065146\n",
      " 0.05126319 0.07531049 0.05867839 0.0654317  0.05699374 0.0829287\n",
      " 0.19679143 0.12635398 0.15482323 0.05769871 0.0547337  0.06025621\n",
      " 0.06046855 0.04486963 0.07722437 0.05649779 0.07000562 0.07286752\n",
      " 0.0646577  0.08973817 0.05453888 0.01478845 0.06684177 0.61753103\n",
      " 0.47918579 0.41718924 0.45236397 0.10176633 0.22515605 0.20582132\n",
      " 0.18490973 0.22130129 0.33771849 0.32673422 0.20582132 0.19911299\n",
      " 0.16166534 0.12525185 0.16757658 0.28425788 0.11003882 0.1309087\n",
      " 0.15174642 0.23449329 0.12634949 0.13430287 0.14010419 0.09985787\n",
      " 0.10188261 0.10746264 0.10161292 0.12758158 0.16336711 0.1271641\n",
      " 0.16233222 0.07711196 0.21207081 0.09438923 0.0946135  0.02559403\n",
      " 0.11050231 0.65179167 0.74910638 0.11835223 0.56965145 0.45120308\n",
      " 0.42338348 0.32383748 0.15690999 0.24559004 0.06689087 0.31365446\n",
      " 0.53242155 0.47338841 0.39372704 0.41460803 0.20077206 0.15281072\n",
      " 0.31913848 0.32117226 0.35096511 0.3003135  0.31143962 0.28045464\n",
      " 0.29203729 0.46076651 0.22162958 0.15106107 0.25380316 0.18033211\n",
      " 0.18164465 0.09150448 1.        ], shape=(669,), dtype=float64), dense_shape=tf.Tensor([  50 4936], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "coo = X_train_bert.tocoo()\n",
    "indices = np.mat([coo.row, coo.col]).transpose()\n",
    "print(tf.SparseTensor(indices, coo.data, coo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_berti = reorder(convert_sparse_matrix_to_sparse_tensor(X_train_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.sparse_tensor.SparseTensor"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_berti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 4936), dtype=float64, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(X_train_berti, validate_indices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 796, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1733, in call  *\n            inputs = input_processing(\n        File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 447, in input_processing  *\n            raise ValueError(\n    \n        ValueError: Data of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.file_utils.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>) is accepted for input_ids.\n    \n    \n    Call arguments received:\n      • input_ids=<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fdb4741eac0>\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_47987/1698690282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_berti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bert\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Local/Thesis/App/.thesis/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 796, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1733, in call  *\n            inputs = input_processing(\n        File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 447, in input_processing  *\n            raise ValueError(\n    \n        ValueError: Data of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.file_utils.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>) is accepted for input_ids.\n    \n    \n    Call arguments received:\n      • input_ids=<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fdb4741eac0>\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(X_train_berti, y_train_bert,  epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing BERT from scratch, without tf-idf data -> will use BERT tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_pickle(\"./data/cleaned_submissions.pkl\")\n",
    "data = data.loc[data[\"sentiment\"] != \"\", [\"text\", \"sentiment\"]] # Only get labeled instances\n",
    "text = data[\"text\"][:50]\n",
    "labels = data[\"sentiment\"][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  370\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(sent) for sent in text])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import re\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=372,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  372\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train data and test data\n",
    "all_posts = np.concatenate([X_train, X_val])\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_posts = [tokenizer.encode(sent, add_special_tokens=True) for sent in text]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_posts])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_45716/2255673211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print sentence 0 and its encoded token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Token IDs: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_45716/2721943929.py\u001b[0m in \u001b[0;36mpreprocessing_for_bert\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#    (6) Return a dictionary of outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         encoded_sent = tokenizer.encode_plus(\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Preprocess sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# Add `[CLS]` and `[SEP]`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m372\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# Max length to truncate/pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_45716/2721943929.py\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Remove '@name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(@.*?)[\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Replace '&amp;' with '&'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_45716/3742937216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run function `preprocessing_for_bert` on the train set and the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tokenizing data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_45716/2721943929.py\u001b[0m in \u001b[0;36mpreprocessing_for_bert\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#    (6) Return a dictionary of outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         encoded_sent = tokenizer.encode_plus(\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Preprocess sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# Add `[CLS]` and `[SEP]`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m372\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# Max length to truncate/pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_45716/2721943929.py\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Remove '@name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(@.*?)[\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Replace '&amp;' with '&'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a199850fbe9b2f1658a16eea735881451fc009eba53b7dda86327ce82228d5dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.thesis': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
