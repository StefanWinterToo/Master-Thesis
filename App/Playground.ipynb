{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Data From Reddit Pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pmaw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/7b1hkdr172d2q0xs5rpt2t680000gn/T/ipykernel_71367/817219406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mReddit_Loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Local/Thesis/App/Reddit_Loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpmaw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushshiftAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDate_Converter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Loads data from Pushshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pmaw'"
     ]
    }
   ],
   "source": [
    "from Reddit_Loader import Loader\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Start and End Date\n",
    "START = \"01/01/2020\"\n",
    "END = datetime.datetime.now().strftime(\"%d/%m/%Y\") #today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response cache key: 29983c8c40dd541667743b8ea5bd1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179167 result(s) available in Pushshift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:: Success Rate: 80.71% - Requests: 2649 - Batches: 266 - Items Remaining: 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354 result(s) not found in Pushshift\n",
      "Retrieved 178813 submissions from Pushshift.\n"
     ]
    }
   ],
   "source": [
    "# Load data from Pushshift and store it as a dataframe\n",
    "l = Loader(\"gme\", START, END, \"wallstreetbets\")\n",
    "submissions = l.load_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "submissions.to_csv('./data/submissions.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as pickle\n",
    "submissions.to_pickle(\"./data/submissions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_pickle(\"./data/submissions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_awardings, allow_live_comments, author, author_flair_css_class, author_flair_richtext, author_flair_text, author_flair_type, author_fullname, author_patreon_flair, author_premium, awarders, can_mod_post, contest_mode, created_utc, domain, full_link, gildings, id, is_crosspostable, is_meta, is_original_content, is_reddit_media_domain, is_robot_indexable, is_self, is_video, link_flair_background_color, link_flair_css_class, link_flair_richtext, link_flair_template_id, link_flair_text, link_flair_text_color, link_flair_type, locked, media_only, no_follow, num_comments, num_crossposts, over_18, parent_whitelist_status, permalink, pinned, pwls, retrieved_on, score, selftext, send_replies, spoiler, stickied, subreddit, subreddit_id, subreddit_subscribers, subreddit_type, suggested_sort, thumbnail, title, total_awards_received, treatment_tags, upvote_ratio, url, whitelist_status, wls, post_hint, preview, removed_by_category, is_gallery, thumbnail_height, thumbnail_width, url_overridden_by_dest, media_metadata, media, media_embed, secure_media, secure_media_embed, author_flair_background_color, author_flair_text_color, edited, gallery_data, banned_by, author_cakeday, discussion_type, crosspost_parent, crosspost_parent_list, author_flair_template_id, author_is_blocked, is_created_from_ads_ui, steward_reports, gilded, distinguished, collections, "
     ]
    }
   ],
   "source": [
    "# All the available columns in the submissions dataframe\n",
    "for element in submissions:\n",
    "    print(element, end = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = (\"all_awardings\",\n",
    "\"allow_live_comments\",\n",
    "\"author\",\n",
    "\"author_flair_css_class\",\n",
    "\"author_flair_richtext\",\n",
    "\"author_flair_text\",\n",
    "\"author_flair_type\",\n",
    "\"author_fullname\",\n",
    "\"author_patreon_flair\",\n",
    "\"author_premium\",\n",
    "\"awarders\",\n",
    "\"can_mod_post\",\n",
    "\"contest_mode\",\n",
    "\"created_utc\",\n",
    "\"domain\",\n",
    "\"full_link\",\n",
    "\"gildings\",\n",
    "\"id\",\n",
    "\"is_crosspostable\",\n",
    "\"is_meta\",\n",
    "\"is_original_content\",\n",
    "\"is_reddit_media_domain\",\n",
    "\"is_robot_indexable\",\n",
    "\"is_self\",\n",
    "\"is_video\",\n",
    "\"link_flair_background_color\",\n",
    "\"link_flair_css_class\",\n",
    "\"link_flair_richtext\",\n",
    "\"link_flair_template_id\",\n",
    "\"link_flair_text\",\n",
    "\"link_flair_text_color\",\n",
    "\"link_flair_type\",\n",
    "\"locked\",\n",
    "\"media_only\",\n",
    "\"no_follow\",\n",
    "\"num_comments\",\n",
    "\"num_crossposts\",\n",
    "\"over_18\",\n",
    "\"parent_whitelist_status\",\n",
    "\"permalink\",\n",
    "\"pinned\",\n",
    "\"pwls\",\n",
    "\"retrieved_on\",\n",
    "\"score\",\n",
    "\"selftext\",\n",
    "\"send_replies\",\n",
    "\"spoiler\",\n",
    "\"stickied\",\n",
    "\"subreddit\",\n",
    "\"subreddit_id\",\n",
    "\"subreddit_subscribers\",\n",
    "\"subreddit_type\",\n",
    "\"suggested_sort\",\n",
    "\"thumbnail\",\n",
    "\"title\",\n",
    "\"total_awards_received\",\n",
    "\"treatment_tags\",\n",
    "\"upvote_ratio\",\n",
    "\"url\",\n",
    "\"whitelist_status\",\n",
    "\"wls\",\n",
    "\"post_hint\",\n",
    "\"preview\",\n",
    "\"removed_by_category\",\n",
    "\"is_gallery\",\n",
    "\"thumbnail_height\",\n",
    "\"thumbnail_width\",\n",
    "\"url_overridden_by_dest\",\n",
    "\"media_metadata\",\n",
    "\"media\",\n",
    "\"media_embed\",\n",
    "\"secure_media\",\n",
    "\"secure_media_embed\",\n",
    "\"author_flair_background_color\",\n",
    "\"author_flair_text_color\",\n",
    "\"edited\",\n",
    "\"gallery_data\",\n",
    "\"banned_by\",\n",
    "\"author_cakeday\",\n",
    "\"discussion_type\",\n",
    "\"crosspost_parent\",\n",
    "\"crosspost_parent_list\",\n",
    "\"author_flair_template_id\",\n",
    "\"author_is_blocked\",\n",
    "\"is_created_from_ads_ui\",\n",
    "\"steward_reports\",\n",
    "\"gilded\",\n",
    "\"distinguished\",\n",
    "\"collections\")\n",
    "\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# The number of Awards can be used as a proxy for the awards given\n",
    "print(len(submissions[\"all_awardings\"][44][0][\"resized_icons\"])) # We only count the unique awards given, not if an award has been given multiple times. May figure it out later.\n",
    "\n",
    "# is_video can be useful to weed out posts, that do not have any content and only a video. Might still be able to perform sentiment analysis on the title.\n",
    "# link_flair_text can be useful to classify content: Meme, Yolo, etc.\n",
    "# subreddit_subscribers is a cool statistic\n",
    "# thumbnail can be used to see if posts with a thumbnail have a bigger impact\n",
    "# media refers to external links. Can probably remove all those entries\n",
    "\n",
    "# To do:\n",
    "# double check the upvote ratio, it seems to be wrong\n",
    "# see how accurate removed_by_category is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "## allow_live_comments is Boolean\n",
    "## author_flair_css_class shows the css class of the author_flair\n",
    "## author_flair_richtext, author_flair_text, author_flair_type can be ignored, because the author can assign a flair to himself if he wants to\n",
    "## author_fullname is just the id for the user\n",
    "## author_patreon_flair, author_premium, domain, gildings, is_crosspostable, is_original_content, is_reddit_media_domain, is_robot_indexable,\n",
    "## is_self, link_flair_background_color, link_flair_text_color, link_flair_css_class, link_flair_template_id, locked, media_only,\n",
    "## no_follow, over_18, parent_whitelist_status, permalink, pwls, send_replies, spoiler, stickied, suggested_sort, subreddit_type, treatment_tags,\n",
    "## whitelist_status, wls, preview, is_gallery, thumbnail_height, thumbnail_width, url_overridden_by_dest, media_metadata, secure_media,\n",
    "## secure_media_embed, author_flair_background_color, author_flair_text_color, edited, gallery_data, banned_by do not seem to be helpful for sentiment analysis\n",
    "## awarders is always empty\n",
    "## can_mod_post, contest_mode, is_meta, pinned are always False\n",
    "## link_flair_type, discussion_type, crosspost_parent, crosspost_parent_list, author_flair_template_id, author_is_blocked,\n",
    "## is_created_from_ads_ui, steward_reports, gilded, distinguished, collections don't really say anything\n",
    "## link_flair_richtext is a dictionary for for link_flair(which is not discarded)\n",
    "## total_awards_received is similar to all_awardings, but not accurate\n",
    "submissions = submissions.drop(columns = [\"allow_live_comments\", \"author_flair_css_class\", \"author_flair_richtext\", \n",
    "                            \"author_flair_text\", \"author_flair_type\", \"author_fullname\", \"author_patreon_flair\",\n",
    "                            \"author_premium\", \"domain\", \"awarders\", \"can_mod_post\", \"contest_mode\",\n",
    "                            \"gildings\", \"is_crosspostable\", \"is_meta\", \"is_original_content\", \"is_reddit_media_domain\",\n",
    "                            \"is_robot_indexable\", \"is_self\", \"link_flair_background_color\", \"link_flair_text_color\",\n",
    "                            \"link_flair_type\", \"link_flair_css_class\", \"link_flair_richtext\", \"link_flair_template_id\",\n",
    "                            \"locked\", \"media_only\", \"no_follow\", \"over_18\", \"parent_whitelist_status\", \"permalink\",\n",
    "                            \"pinned\", \"pwls\", \"send_replies\", \"spoiler\", \"stickied\", \"subreddit_type\", \"suggested_sort\",\n",
    "                            \"treatment_tags\", \"whitelist_status\", \"wls\", \"preview\", \"is_gallery\", \"thumbnail_height\",\n",
    "                            \"thumbnail_width\", \"url_overridden_by_dest\", \"media_metadata\", \"secure_media\", \"secure_media_embed\",\n",
    "                            \"author_flair_background_color\", \"author_flair_text_color\", \"edited\", \"gallery_data\",\n",
    "                            \"banned_by\", \"discussion_type\", \"crosspost_parent\", \"crosspost_parent_list\",\n",
    "                            \"author_flair_template_id\", \"author_is_blocked\", \"is_created_from_ads_ui\",\n",
    "                            \"steward_reports\", \"gilded\", \"distinguished\", \"collections\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['all_awardings', 'author', 'created_utc', 'full_link', 'id', 'is_video',\n",
      "       'num_comments', 'num_crossposts', 'retrieved_on', 'score', 'selftext',\n",
      "       'subreddit', 'subreddit_id', 'subreddit_subscribers', 'thumbnail',\n",
      "       'title', 'total_awards_received', 'upvote_ratio', 'url',\n",
      "       'link_flair_text', 'post_hint', 'removed_by_category', 'author_cakeday',\n",
      "       'media', 'media_embed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(submissions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[\"missing_content\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.loc[(submissions[\"selftext\"] == \"[removed]\"), \"missing_content\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.loc[(submissions[\"selftext\"] == \"[deleted]\"), \"missing_content\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_content = []\n",
    "for i, text in enumerate(submissions[\"selftext\"]):\n",
    "    if(len(str(text)) == 0):\n",
    "        empty_content.append(i)\n",
    "\n",
    "submissions.loc[empty_content, \"missing_content\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     135828\n",
       "False     43716\n",
       "Name: missing_content, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions[\"missing_content\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86918</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      selftext\n",
       "86918      NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"Lol I just joined today and bought GME at $90. What are yall doing?\"\n",
    "submissions[submissions[\"title\"] == title][[\"selftext\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.loc[(submissions[\"selftext\"].isnull() == True), \"missing_content\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Powerlaw\n",
    "Almost 80% of posts were deleted or contain no content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with text: 37966\n",
      "Posts with removed text: 141578\n",
      "Proportion of posts with removed text: 0.79\n"
     ]
    }
   ],
   "source": [
    "ful_text = len(submissions[submissions[\"missing_content\"] == False][\"selftext\"])\n",
    "rem_text = len(submissions[submissions[\"missing_content\"] == True][\"selftext\"])\n",
    "\n",
    "# How many posts have the text not removed?\n",
    "print(\"Posts with text:\", ful_text)\n",
    "\n",
    "# How many posts have the text \"removed\"?\n",
    "print(\"Posts with removed text:\", rem_text)\n",
    "\n",
    "print(\"Proportion of posts with removed text:\", round((rem_text)/(rem_text+ful_text),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed by:\n",
      " selftext   removed_by_category\n",
      "[removed]  moderator              64059\n",
      "           reddit                  2201\n",
      "           automod_filtered         620\n",
      "           author                     1\n",
      "dtype: int64\n",
      "---------------------------------------------------\n",
      "Removed with x_comments:\n",
      " selftext   num_comments\n",
      "[removed]  0               52176\n",
      "           2                8593\n",
      "           1                1969\n",
      "           4                 955\n",
      "           3                 582\n",
      "                           ...  \n",
      "           192                 1\n",
      "           191                 1\n",
      "           190                 1\n",
      "           186                 1\n",
      "           64470               1\n",
      "Length: 211, dtype: int64\n",
      "---------------------------------------------------\n",
      "Removed with x_comments:\n",
      " selftext   score\n",
      "[removed]  1        60773\n",
      "           2         1516\n",
      "           0         1412\n",
      "           3          459\n",
      "           4          261\n",
      "                    ...  \n",
      "           180          1\n",
      "           179          1\n",
      "           173          1\n",
      "           170          1\n",
      "           42345        1\n",
      "Length: 217, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Quite a lot of posts got removed. This is unfortunate. However, the title is still available. So I might still use the title.\n",
    "\n",
    "# Who usually removes the post?\n",
    "print(\"Removed by:\\n\", submissions[submissions[\"selftext\"] == \"[removed]\"][[\"selftext\", \"removed_by_category\"]].value_counts())\n",
    "print(\"-----------------\"*3)\n",
    "# Were there any posts with a lot of comments that were removed?\n",
    "print(\"Removed with x_comments:\\n\", submissions[submissions[\"selftext\"] == \"[removed]\"][[\"selftext\", \"num_comments\"]].value_counts())\n",
    "print(\"-----------------\"*3)\n",
    "# Were there any posts with a high score that were removed?\n",
    "print(\"Removed with x_comments:\\n\", submissions[submissions[\"selftext\"] == \"[removed]\"][[\"selftext\", \"score\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put removed submissions into its own dataframe and then only use submissions that still exist.\n",
    "submissions_removed = submissions[submissions[\"selftext\"] == \"[removed]\"]\n",
    "submissions = submissions[submissions[\"selftext\"] != \"[removed]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submissions with flag to csv to do some manual analysis\n",
    "submissions.to_csv('./data/test.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://realpython.com/nltk-nlp-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tokenizing by word:</b> Words are like the atoms of natural language. They’re the smallest unit of meaning that still makes sense on its own. Tokenizing your text by word allows you to identify words that come up particularly often. For example, if you were analyzing a group of job ads, then you might find that the word “Python” comes up often. That could suggest high demand for Python knowledge, but you’d need to look deeper to know more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tokenizing by sentence:</b> When you tokenize by sentence, you can analyze how those words relate to one another and see more context. Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an annotation tool for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Tk, Button, Frame, Text, messagebox\n",
    "import xlsxwriter # cannot modify excel files\n",
    "from openpyxl import load_workbook # can modify excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have close to 180k entries.\n",
    "# If I take 2 seconds per annotation I can annotate 18k titles in 10 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_new_annotation_excel_file():\\n    workbook = xlsxwriter.Workbook(\\'./data/annotation.xlsx\\')\\n    worksheet = workbook.add_worksheet()\\n\\n    for i in range(0, len(submissions[\"title\"]), 10):\\n        # row, column, item\\n        worksheet.write(int(i/10), 0, i)\\n        worksheet.write(int(i/10), 1, submissions[\"title\"][i])\\n        worksheet.write(int(i/10), 2, \"?\")\\n\\n    workbook.close()\\n\\n    b = load_workbook(filename = \"./data/annotation.xlsx\")\\n    ws = wb.active\\n\\n    ws[\"D2\"] = \"Number of annotations:\"\\n    ws[\"D3\"] = 0\\n\\n    wb.save(filename = \"./data/annotation.xlsx\")\\n\\ncreate_new_annotation_excel_file()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates workbook with non annotated data\n",
    "'''\n",
    "def create_new_annotation_excel_file():\n",
    "    workbook = xlsxwriter.Workbook('./data/annotation.xlsx')\n",
    "    worksheet = workbook.add_worksheet()\n",
    "\n",
    "    for i in range(0, len(submissions[\"title\"]), 10):\n",
    "        # row, column, item\n",
    "        worksheet.write(int(i/10), 0, i)\n",
    "        worksheet.write(int(i/10), 1, submissions[\"title\"][i])\n",
    "        worksheet.write(int(i/10), 2, \"?\")\n",
    "\n",
    "    workbook.close()\n",
    "\n",
    "    b = load_workbook(filename = \"./data/annotation.xlsx\")\n",
    "    ws = wb.active\n",
    "\n",
    "    ws[\"D2\"] = \"Number of annotations:\"\n",
    "    ws[\"D3\"] = 0\n",
    "\n",
    "    wb.save(filename = \"./data/annotation.xlsx\")\n",
    "\n",
    "create_new_annotation_excel_file()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "ws[\"D3\"] = 0\n",
    "save_to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation():\n",
    "    wb = load_workbook(filename = \"./data/annotation.xlsx\")\n",
    "    ws = wb.active\n",
    "    return ws, wb\n",
    "\n",
    "ws, wb = load_annotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_count():\n",
    "    i = ws[\"D3\"].value\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_annotation_count():\n",
    "    i = get_annotation_count()\n",
    "    i = i + 1\n",
    "    ws[\"D3\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "def save_to_excel():\n",
    "    wb.save(filename = \"./data/annotation.xlsx\")\n",
    "    print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_annotations():\n",
    "    x = get_annotation_count()\n",
    "    m = len(ws['A'])\n",
    "\n",
    "    return str(x) + \" / \" + str(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bullish_func_button():\n",
    "    increment_annotation_count()\n",
    "    row = get_annotation_count() + 1\n",
    "    cell = \"C\" + str(row)\n",
    "    ws[cell] = \"bullish\"\n",
    "\n",
    "def bearish_func_button():\n",
    "    increment_annotation_count()\n",
    "    row = get_annotation_count() + 1\n",
    "    cell = \"C\" + str(row)\n",
    "    ws[cell] = \"bearish\"\n",
    "\n",
    "def neutral_func_button():\n",
    "    increment_annotation_count()\n",
    "    row = get_annotation_count() + 1\n",
    "    cell = \"C\" + str(row)\n",
    "    ws[cell] = \"neutral\"\n",
    "\n",
    "def bullish_func(self):\n",
    "   bullish_func_button()\n",
    "   gui.title(count_annotations())\n",
    "   update_label()\n",
    "\n",
    "def bearish_func(self):\n",
    "   bearish_func_button()\n",
    "   gui.title(count_annotations())\n",
    "   update_label()\n",
    "\n",
    "def neutral_func(self):\n",
    "    neutral_func_button()\n",
    "    gui.title(count_annotations())\n",
    "    update_label()\n",
    "\n",
    "def on_saving():\n",
    "    save_to_excel()\n",
    "    return \"\"\n",
    "\n",
    "def update_label():\n",
    "    row = get_annotation_count() + 1\n",
    "    cell = \"B\" + str(row)\n",
    "    label.config(text=ws[cell].value)\n",
    "\n",
    "gui = tk.Tk()\n",
    "\n",
    "BUTTON_HEIGHT = 3\n",
    "BUTTON_WIDTH = 10\n",
    "\n",
    "gui.config(bg = \"#222222\")\n",
    "gui.geometry(\"500x500\")\n",
    "gui.title(count_annotations())\n",
    "\n",
    "# Bullish button\n",
    "bullish = Button(gui, text = \"bullish <a>\", command = bullish_func_button)\n",
    "gui.bind('a', bullish_func)\n",
    "bullish.config(height = BUTTON_HEIGHT, width = BUTTON_WIDTH)\n",
    "bullish.place(x=5,y=440)\n",
    "\n",
    "# Bearish button\n",
    "bearish = Button(gui, text = \"bearish <b>\", command = bearish_func_button)\n",
    "bearish.config(height = BUTTON_HEIGHT, width = BUTTON_WIDTH)\n",
    "gui.bind('b', bearish_func)\n",
    "bearish.place(x=415,y=440)\n",
    "\n",
    "# Neutral button\n",
    "neutral = Button(gui, text = \"Neutral < >\", command = neutral_func_button)\n",
    "neutral.config(height = BUTTON_HEIGHT, width = BUTTON_WIDTH)\n",
    "gui.bind('<space>', neutral_func)\n",
    "neutral.place(x=205,y=440)\n",
    "\n",
    "# Text    \n",
    "row = get_annotation_count() + 1\n",
    "cell = \"B\" + str(row)\n",
    "label = tk.Label(gui, justify = tk.CENTER, pady = 10, wraplengt=400, font = (\"Arial\", 15), text = ws[cell].value)\n",
    "label.pack(pady = 50)\n",
    "\n",
    "# Menu\n",
    "menubar = tk.Menu(gui)\n",
    "filemenu = tk.Menu(menubar, tearoff=0)\n",
    "filemenu.add_command(label=\"Save\", command=on_saving)\n",
    "menubar.add_cascade(label=\"File\", menu=filemenu)\n",
    "gui.config(menu=menubar)\n",
    "\n",
    "gui.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
