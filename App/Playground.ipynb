{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Data From Reddit Pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reddit_Loader import Loader\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Start and End Date\n",
    "START = \"01/01/2020\"\n",
    "END = datetime.datetime.now().strftime(\"%d/%m/%Y\") #today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response cache key: 29983c8c40dd541667743b8ea5bd1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179167 result(s) available in Pushshift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:: Success Rate: 80.71% - Requests: 2649 - Batches: 266 - Items Remaining: 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354 result(s) not found in Pushshift\n",
      "Retrieved 178813 submissions from Pushshift.\n"
     ]
    }
   ],
   "source": [
    "# Load data from Pushshift and store it as a dataframe\n",
    "l = Loader(\"gme\", START, END, \"wallstreetbets\")\n",
    "submissions = l.load_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "submissions.to_csv('./data/submissions.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as pickle\n",
    "submissions.to_pickle(\"./data/submissions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_pickle(\"./data/submissions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_awardings, allow_live_comments, author, author_flair_css_class, author_flair_richtext, author_flair_text, author_flair_type, author_fullname, author_patreon_flair, author_premium, awarders, can_mod_post, contest_mode, created_utc, domain, full_link, gildings, id, is_crosspostable, is_meta, is_original_content, is_reddit_media_domain, is_robot_indexable, is_self, is_video, link_flair_background_color, link_flair_css_class, link_flair_richtext, link_flair_template_id, link_flair_text, link_flair_text_color, link_flair_type, locked, media_only, no_follow, num_comments, num_crossposts, over_18, parent_whitelist_status, permalink, pinned, pwls, retrieved_on, score, selftext, send_replies, spoiler, stickied, subreddit, subreddit_id, subreddit_subscribers, subreddit_type, suggested_sort, thumbnail, title, total_awards_received, treatment_tags, upvote_ratio, url, whitelist_status, wls, post_hint, preview, removed_by_category, is_gallery, thumbnail_height, thumbnail_width, url_overridden_by_dest, media_metadata, media, media_embed, secure_media, secure_media_embed, author_flair_background_color, author_flair_text_color, edited, gallery_data, banned_by, author_cakeday, discussion_type, crosspost_parent, crosspost_parent_list, author_flair_template_id, author_is_blocked, is_created_from_ads_ui, steward_reports, gilded, distinguished, collections, "
     ]
    }
   ],
   "source": [
    "# All the available columns in the submissions dataframe\n",
    "for element in submissions:\n",
    "    print(element, end = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = (\"all_awardings\",\n",
    "\"allow_live_comments\",\n",
    "\"author\",\n",
    "\"author_flair_css_class\",\n",
    "\"author_flair_richtext\",\n",
    "\"author_flair_text\",\n",
    "\"author_flair_type\",\n",
    "\"author_fullname\",\n",
    "\"author_patreon_flair\",\n",
    "\"author_premium\",\n",
    "\"awarders\",\n",
    "\"can_mod_post\",\n",
    "\"contest_mode\",\n",
    "\"created_utc\",\n",
    "\"domain\",\n",
    "\"full_link\",\n",
    "\"gildings\",\n",
    "\"id\",\n",
    "\"is_crosspostable\",\n",
    "\"is_meta\",\n",
    "\"is_original_content\",\n",
    "\"is_reddit_media_domain\",\n",
    "\"is_robot_indexable\",\n",
    "\"is_self\",\n",
    "\"is_video\",\n",
    "\"link_flair_background_color\",\n",
    "\"link_flair_css_class\",\n",
    "\"link_flair_richtext\",\n",
    "\"link_flair_template_id\",\n",
    "\"link_flair_text\",\n",
    "\"link_flair_text_color\",\n",
    "\"link_flair_type\",\n",
    "\"locked\",\n",
    "\"media_only\",\n",
    "\"no_follow\",\n",
    "\"num_comments\",\n",
    "\"num_crossposts\",\n",
    "\"over_18\",\n",
    "\"parent_whitelist_status\",\n",
    "\"permalink\",\n",
    "\"pinned\",\n",
    "\"pwls\",\n",
    "\"retrieved_on\",\n",
    "\"score\",\n",
    "\"selftext\",\n",
    "\"send_replies\",\n",
    "\"spoiler\",\n",
    "\"stickied\",\n",
    "\"subreddit\",\n",
    "\"subreddit_id\",\n",
    "\"subreddit_subscribers\",\n",
    "\"subreddit_type\",\n",
    "\"suggested_sort\",\n",
    "\"thumbnail\",\n",
    "\"title\",\n",
    "\"total_awards_received\",\n",
    "\"treatment_tags\",\n",
    "\"upvote_ratio\",\n",
    "\"url\",\n",
    "\"whitelist_status\",\n",
    "\"wls\",\n",
    "\"post_hint\",\n",
    "\"preview\",\n",
    "\"removed_by_category\",\n",
    "\"is_gallery\",\n",
    "\"thumbnail_height\",\n",
    "\"thumbnail_width\",\n",
    "\"url_overridden_by_dest\",\n",
    "\"media_metadata\",\n",
    "\"media\",\n",
    "\"media_embed\",\n",
    "\"secure_media\",\n",
    "\"secure_media_embed\",\n",
    "\"author_flair_background_color\",\n",
    "\"author_flair_text_color\",\n",
    "\"edited\",\n",
    "\"gallery_data\",\n",
    "\"banned_by\",\n",
    "\"author_cakeday\",\n",
    "\"discussion_type\",\n",
    "\"crosspost_parent\",\n",
    "\"crosspost_parent_list\",\n",
    "\"author_flair_template_id\",\n",
    "\"author_is_blocked\",\n",
    "\"is_created_from_ads_ui\",\n",
    "\"steward_reports\",\n",
    "\"gilded\",\n",
    "\"distinguished\",\n",
    "\"collections\")\n",
    "\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# The number of Awards can be used as a proxy for the awards given\n",
    "print(len(submissions[\"all_awardings\"][44][0][\"resized_icons\"])) # We only count the unique awards given, not if an award has been given multiple times. May figure it out later.\n",
    "\n",
    "# is_video can be useful to weed out posts, that do not have any content and only a video. Might still be able to perform sentiment analysis on the title.\n",
    "# link_flair_text can be useful to classify content: Meme, Yolo, etc.\n",
    "# subreddit_subscribers is a cool statistic\n",
    "# thumbnail can be used to see if posts with a thumbnail have a bigger impact\n",
    "# media refers to external links. Can probably remove all those entries\n",
    "\n",
    "# To do:\n",
    "# double check the upvote ratio, it seems to be wrong\n",
    "# see how accurate removed_by_category is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "## allow_live_comments is Boolean\n",
    "## author_flair_css_class shows the css class of the author_flair\n",
    "## author_flair_richtext, author_flair_text, author_flair_type can be ignored, because the author can assign a flair to himself if he wants to\n",
    "## author_fullname is just the id for the user\n",
    "## author_patreon_flair, author_premium, domain, gildings, is_crosspostable, is_original_content, is_reddit_media_domain, is_robot_indexable,\n",
    "## is_self, link_flair_background_color, link_flair_text_color, link_flair_css_class, link_flair_template_id, locked, media_only,\n",
    "## no_follow, over_18, parent_whitelist_status, permalink, pwls, send_replies, spoiler, stickied, suggested_sort, subreddit_type, treatment_tags,\n",
    "## whitelist_status, wls, preview, is_gallery, thumbnail_height, thumbnail_width, url_overridden_by_dest, media_metadata, secure_media,\n",
    "## secure_media_embed, author_flair_background_color, author_flair_text_color, edited, gallery_data, banned_by do not seem to be helpful for sentiment analysis\n",
    "## awarders is always empty\n",
    "## can_mod_post, contest_mode, is_meta, pinned are always False\n",
    "## link_flair_type, discussion_type, crosspost_parent, crosspost_parent_list, author_flair_template_id, author_is_blocked,\n",
    "## is_created_from_ads_ui, steward_reports, gilded, distinguished, collections don't really say anything\n",
    "## link_flair_richtext is a dictionary for for link_flair(which is not discarded)\n",
    "## total_awards_received is similar to all_awardings, but not accurate\n",
    "submissions = submissions.drop(columns = [\"allow_live_comments\", \"author_flair_css_class\", \"author_flair_richtext\", \n",
    "                            \"author_flair_text\", \"author_flair_type\", \"author_fullname\", \"author_patreon_flair\",\n",
    "                            \"author_premium\", \"domain\", \"awarders\", \"can_mod_post\", \"contest_mode\",\n",
    "                            \"gildings\", \"is_crosspostable\", \"is_meta\", \"is_original_content\", \"is_reddit_media_domain\",\n",
    "                            \"is_robot_indexable\", \"is_self\", \"link_flair_background_color\", \"link_flair_text_color\",\n",
    "                            \"link_flair_type\", \"link_flair_css_class\", \"link_flair_richtext\", \"link_flair_template_id\",\n",
    "                            \"locked\", \"media_only\", \"no_follow\", \"over_18\", \"parent_whitelist_status\", \"permalink\",\n",
    "                            \"pinned\", \"pwls\", \"send_replies\", \"spoiler\", \"stickied\", \"subreddit_type\", \"suggested_sort\",\n",
    "                            \"treatment_tags\", \"whitelist_status\", \"wls\", \"preview\", \"is_gallery\", \"thumbnail_height\",\n",
    "                            \"thumbnail_width\", \"url_overridden_by_dest\", \"media_metadata\", \"secure_media\", \"secure_media_embed\",\n",
    "                            \"author_flair_background_color\", \"author_flair_text_color\", \"edited\", \"gallery_data\",\n",
    "                            \"banned_by\", \"discussion_type\", \"crosspost_parent\", \"crosspost_parent_list\",\n",
    "                            \"author_flair_template_id\", \"author_is_blocked\", \"is_created_from_ads_ui\",\n",
    "                            \"steward_reports\", \"gilded\", \"distinguished\", \"collections\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['all_awardings', 'author', 'created_utc', 'full_link', 'id', 'is_video',\n",
      "       'num_comments', 'num_crossposts', 'retrieved_on', 'score', 'selftext',\n",
      "       'subreddit', 'subreddit_id', 'subreddit_subscribers', 'thumbnail',\n",
      "       'title', 'total_awards_received', 'upvote_ratio', 'url',\n",
      "       'link_flair_text', 'post_hint', 'removed_by_category', 'author_cakeday',\n",
      "       'media', 'media_embed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(submissions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[\"missing_content\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.loc[(submissions[\"selftext\"] == \"[removed]\"), \"missing_content\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.loc[(submissions[\"selftext\"] == \"[deleted]\"), \"missing_content\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_content = []\n",
    "for i, text in enumerate(submissions[\"selftext\"]):\n",
    "    if(len(str(text)) == 0):\n",
    "        empty_content.append(i)\n",
    "\n",
    "submissions.loc[empty_content, \"missing_content\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     135828\n",
       "False     43716\n",
       "Name: missing_content, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions[\"missing_content\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86918</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      selftext\n",
       "86918      NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"Lol I just joined today and bought GME at $90. What are yall doing?\"\n",
    "submissions[submissions[\"title\"] == title][[\"selftext\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Powerlaw\n",
    "Almost 80% of posts were deleted or contain no content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with text: 37966\n",
      "Posts with removed text: 141578\n",
      "Proportion of posts with removed text: 0.79\n"
     ]
    }
   ],
   "source": [
    "ful_text = len(submissions[submissions[\"missing_content\"] == False][\"selftext\"])\n",
    "rem_text = len(submissions[submissions[\"missing_content\"] == True][\"selftext\"])\n",
    "\n",
    "# How many posts have the text not removed?\n",
    "print(\"Posts with text:\", ful_text)\n",
    "\n",
    "# How many posts have the text \"removed\"?\n",
    "print(\"Posts with removed text:\", rem_text)\n",
    "\n",
    "print(\"Proportion of posts with removed text:\", round((rem_text)/(rem_text+ful_text),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed by:\n",
      " selftext   removed_by_category\n",
      "[removed]  moderator              64059\n",
      "           reddit                  2201\n",
      "           automod_filtered         620\n",
      "           author                     1\n",
      "dtype: int64\n",
      "---------------------------------------------------\n",
      "Removed with x_comments:\n",
      " selftext   num_comments\n",
      "[removed]  0               52176\n",
      "           2                8593\n",
      "           1                1969\n",
      "           4                 955\n",
      "           3                 582\n",
      "                           ...  \n",
      "           192                 1\n",
      "           191                 1\n",
      "           190                 1\n",
      "           186                 1\n",
      "           64470               1\n",
      "Length: 211, dtype: int64\n",
      "---------------------------------------------------\n",
      "Removed with x_comments:\n",
      " selftext   score\n",
      "[removed]  1        60773\n",
      "           2         1516\n",
      "           0         1412\n",
      "           3          459\n",
      "           4          261\n",
      "                    ...  \n",
      "           180          1\n",
      "           179          1\n",
      "           173          1\n",
      "           170          1\n",
      "           42345        1\n",
      "Length: 217, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Quite a lot of posts got removed. This is unfortunate. However, the title is still available. So I might still use the title.\n",
    "\n",
    "# Who usually removes the post?\n",
    "print(\"Removed by:\\n\", submissions[submissions[\"selftext\"] == \"[removed]\"][[\"selftext\", \"removed_by_category\"]].value_counts())\n",
    "print(\"-----------------\"*3)\n",
    "# Were there any posts with a lot of comments that were removed?\n",
    "print(\"Removed with x_comments:\\n\", submissions[submissions[\"selftext\"] == \"[removed]\"][[\"selftext\", \"num_comments\"]].value_counts())\n",
    "print(\"-----------------\"*3)\n",
    "# Were there any posts with a high score that were removed?\n",
    "print(\"Removed with x_comments:\\n\", submissions[submissions[\"selftext\"] == \"[removed]\"][[\"selftext\", \"score\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put removed submissions into its own dataframe and then only use submissions that still exist.\n",
    "submissions_removed = submissions[submissions[\"selftext\"] == \"[removed]\"]\n",
    "submissions = submissions[submissions[\"selftext\"] != \"[removed]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submissions with flag to csv to do some manual analysis\n",
    "submissions.to_csv('./data/test.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://realpython.com/nltk-nlp-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tokenizing by word:</b> Words are like the atoms of natural language. They’re the smallest unit of meaning that still makes sense on its own. Tokenizing your text by word allows you to identify words that come up particularly often. For example, if you were analyzing a group of job ads, then you might find that the word “Python” comes up often. That could suggest high demand for Python knowledge, but you’d need to look deeper to know more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tokenizing by sentence:</b> When you tokenize by sentence, you can analyze how those words relate to one another and see more context. Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read in seed data and concatenate the title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = pd.read_excel(\"./data/annotation.xlsx\", header = None, usecols = range(0, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "seed = seed.rename(columns = {0 : \"index\", 1 : \"title\", 2 : \"content\", 3 : \"sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "bullish      9357\n",
       "neutral      5119\n",
       "bearish      3479\n",
       "dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed[[\"sentiment\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_pickle(\"./data/submissions.pkl\")\n",
    "\n",
    "# Concatenate Text\n",
    "submissions[\"text\"] = submissions[\"title\"] + \" \" + submissions[\"selftext\"]\n",
    "\n",
    "# Drop columns\n",
    "submissions = submissions.drop(columns = {\"title\", \"selftext\"})\n",
    "\n",
    "# Add sentiment column\n",
    "submissions[\"sentiment\"] = \"\"\n",
    "\n",
    "# Add known labels\n",
    "for i in seed[\"index\"]:\n",
    "    submissions.loc[i, \"sentiment\"] = seed.loc[i/10, \"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 2103, in run\n",
      "    for msg in self.data_server.incr_download(self.items):\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 623, in incr_download\n",
      "    yield from self._download_list(info_or_id, download_dir, force)\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 666, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 636, in incr_download\n",
      "    yield from self.incr_download(info.children, download_dir, force)\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 623, in incr_download\n",
      "    yield from self._download_list(info_or_id, download_dir, force)\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 666, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 641, in incr_download\n",
      "    yield from self._download_package(info, download_dir, force)\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 734, in _download_package\n",
      "    for msg in _unzip_iter(filepath, zipdir, verbose=False):\n",
      "  File \"/Users/stefanwinter/Local/Thesis/App/.thesis/lib/python3.9/site-packages/nltk/downloader.py\", line 2249, in _unzip_iter\n",
      "    zf.extractall(root)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/zipfile.py\", line 1633, in extractall\n",
      "    self._extract_member(zipinfo, path, pwd)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/zipfile.py\", line 1688, in _extract_member\n",
      "    shutil.copyfileobj(source, target)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/shutil.py\", line 208, in copyfileobj\n",
      "    fdst_write(buf)\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/Users/stefanwinter/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:301180)",
      "at w.execute (/Users/stefanwinter/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/Users/stefanwinter/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/Users/stefanwinter/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (/Users/stefanwinter/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "submissions['tokenized_column'] = submissions[\"text\"].apply(custom_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "'''learner = ActiveLearner(estimator = SVC(), \\\n",
    "    X_training=)'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a199850fbe9b2f1658a16eea735881451fc009eba53b7dda86327ce82228d5dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.thesis': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
