{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from psaw import PushshiftAPI\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "api = PushshiftAPI()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "gen = api.search_submissions(limit=100)\n",
    "results = list(gen)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/stefanwinter/Library/Python/3.7/lib/python/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Convert date to unix timestampe\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def convert_time_to_unix(START, END):\n",
    "    start = int(time.mktime(datetime.datetime.strptime(START, \"%d/%m/%Y\").timetuple()))\n",
    "    end = int(time.mktime(datetime.datetime.strptime(END, \"%d/%m/%Y\").timetuple()))\n",
    "\n",
    "    return start, end\n",
    "\n",
    "#    print(\"Start: {} -> {}\".format(START, start))\n",
    "#    print(\"End: {} -> {}\".format(END, end))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pushshift Params  \n",
    "size — increase limit of returned entries to 1000  \n",
    "after — where to start the search  \n",
    "before — where to end the search  \n",
    "title — to search only within the submission’s title  \n",
    "subreddit — to narrow it down to a particular subreddit  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def collectSubData(subm):\n",
    "    subData = list() #list to store data points\n",
    "    title = subm['title']\n",
    "    url = subm['url']\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"    \n",
    "    author = subm['author']\n",
    "    sub_id = subm['id']\n",
    "    score = subm['score']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    numComms = subm['num_comments']\n",
    "    permalink = subm['permalink']\n",
    "    \n",
    "    subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair))\n",
    "    subStats[sub_id] = subData"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "START = \"01/09/2021\"\n",
    "END = datetime.datetime.now().strftime(\"%d/%m/%Y\") #today\n",
    "\n",
    "#Subreddit to query\n",
    "sub='wallstreetbets'\n",
    "#before and after dates\n",
    "after, before = convert_time_to_unix(START, END)\n",
    "print(after, before)\n",
    "query = \"gme\"\n",
    "subCount = 0\n",
    "subStats = {}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1630447200 1631138400\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    \n",
    "print(len(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=gme&size=1000&after=1630447200&before=1631138400&subreddit=wallstreetbets\n",
      "100\n",
      "2021-09-04 20:36:34\n",
      "https://api.pushshift.io/reddit/search/submission/?title=gme&size=1000&after=1630780594&before=1631138400&subreddit=wallstreetbets\n",
      "55\n",
      "2021-09-08 07:17:25\n",
      "https://api.pushshift.io/reddit/search/submission/?title=gme&size=1000&after=1631078245&before=1631138400&subreddit=wallstreetbets\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(str(len(subStats)) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "155 submissions have added to list\n",
      "1st entry is:\n",
      "I'm setting my sell limit high for GME so Robinhood can't give it to short sellers at current pricing....as I was told this was the way? Anyway, what dafuq is going on here? I don't want to hear about how bad RH is. I know. But I'm not cashing out and losing money. Well screw that. Any thoughts? created: 2021-09-01 00:15:06\n",
      "Last entry is:\n",
      "Since some of you think you’ll pull some magic out your ass and revive gme created: 2021-09-08 07:17:25\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def updateSubs_file():\n",
    "    upload_count = 0\n",
    "    location = \"data/\"\n",
    "    print(\"input filename of submission file, please add .csv\")\n",
    "    filename = input()\n",
    "    file = location + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
    "        a = csv.writer(file, delimiter=',')\n",
    "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Permalink\",\"Flair\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "updateSubs_file()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input filename of submission file, please add .csv\n",
      "155 submissions have been uploaded\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "## Using the pmaw pushshift api wrapper\n",
    "# https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "\n",
    "api = PushshiftAPI()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "START = \"01/09/2021\"\n",
    "END = datetime.datetime.now().strftime(\"%d/%m/%Y\") #today\n",
    "\n",
    "#Subreddit to query\n",
    "subreddit = 'wallstreetbets'\n",
    "#before and after dates\n",
    "after, before = convert_time_to_unix(START, END)\n",
    "print(after, before)\n",
    "query = \"gme\"\n",
    "subCount = 0\n",
    "subStats = {}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1630447200 1631224800\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# Submissions\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=1000\n",
    "submissions = api.search_submissions(subreddit=subreddit, q = \"gme\", before=before, after=after, mem_safe=True, safe_exit=True)\n",
    "print(f'Retrieved {len(submissions)} comments from Pushshift')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Response cache key: d4780f3a8295132fb947e64afac8469f\n",
      "337 result(s) available in Pushshift\n",
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n",
      "Retrieved 337 comments from Pushshift\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "submissions_df = pd.DataFrame(submissions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "print(submissions_df.columns)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['all_awardings', 'allow_live_comments', 'author',\n",
      "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
      "       'author_flair_type', 'author_fullname', 'author_is_blocked',\n",
      "       'author_patreon_flair', 'author_premium', 'awarders', 'can_mod_post',\n",
      "       'contest_mode', 'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
      "       'is_created_from_ads_ui', 'is_crosspostable', 'is_meta',\n",
      "       'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable',\n",
      "       'is_self', 'is_video', 'link_flair_background_color',\n",
      "       'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id',\n",
      "       'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked',\n",
      "       'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
      "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
      "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
      "       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
      "       'subreddit_type', 'suggested_sort', 'thumbnail', 'title',\n",
      "       'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url',\n",
      "       'whitelist_status', 'wls', 'post_hint', 'preview', 'thumbnail_height',\n",
      "       'thumbnail_width', 'url_overridden_by_dest', 'removed_by_category',\n",
      "       'author_flair_background_color', 'author_flair_text_color',\n",
      "       'media_metadata', 'author_flair_template_id', 'gallery_data',\n",
      "       'is_gallery', 'media', 'media_embed', 'secure_media',\n",
      "       'secure_media_embed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# Retrieve comments by submission id\n",
    "\n",
    "submission_ids = list(submissions_df[\"id\"])\n",
    "\n",
    "c = api.search_submission_comment_ids(ids = submission_ids[:10])\n",
    "\n",
    "c_df = pd.DataFrame(c)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "submissions_df[\"score\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "332    1\n",
       "333    1\n",
       "334    1\n",
       "335    1\n",
       "336    1\n",
       "Name: score, Length: 337, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "print(submission_ids[:10])\n",
    "print(c_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pg0759', 'pg0249', 'pfzgt2', 'pfxslq', 'pfxdnj', 'pfx9de', 'pfx74v', 'pfwgby', 'pfwff1', 'pfwdi4']\n",
      "          0\n",
      "0   hb7g632\n",
      "1   hb7x8cy\n",
      "2   hb7s0n3\n",
      "3   hb7w7ie\n",
      "4   hb7d844\n",
      "5   hb7dfbi\n",
      "6   hb7cgto\n",
      "7   hb7bvs7\n",
      "8   hb7c2xb\n",
      "9   hb76e2g\n",
      "10  hb76guy\n",
      "11  hb76wp1\n",
      "12  hb77l86\n",
      "13  hb780th\n",
      "14  hb784hr\n",
      "15  hb787o0\n",
      "16  hb7889o\n",
      "17  hb78pam\n",
      "18  hb79284\n",
      "19  hb7a0az\n",
      "20  hb7f9zs\n",
      "21  hb7t763\n",
      "22  hb767e9\n",
      "23  hb75vhf\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "print(\"First ten submission ids: \\n\", submission_ids[:10])\n",
    "c_df = pd.DataFrame(c)\n",
    "print(c_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First ten submission ids: \n",
      " ['pg0759', 'pg0249', 'pfzgt2', 'pfxslq', 'pfxdnj', 'pfx9de', 'pfx74v', 'pfwgby', 'pfwff1', 'pfwdi4']\n",
      "          0\n",
      "0   hb7s0n3\n",
      "1   hb7w7ie\n",
      "2   hb7x8cy\n",
      "3   hb7g632\n",
      "4   hb7d844\n",
      "5   hb7dfbi\n",
      "6   hb7cgto\n",
      "7   hb7bvs7\n",
      "8   hb7c2xb\n",
      "9   hb76e2g\n",
      "10  hb76guy\n",
      "11  hb76wp1\n",
      "12  hb77l86\n",
      "13  hb780th\n",
      "14  hb784hr\n",
      "15  hb787o0\n",
      "16  hb7889o\n",
      "17  hb78pam\n",
      "18  hb79284\n",
      "19  hb7a0az\n",
      "20  hb7f9zs\n",
      "21  hb7t763\n",
      "22  hb767e9\n",
      "23  hb75vhf\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Search for all Comments\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=1000\n",
    "comments = api.search_comments(subreddit=subreddit, q = \"gme\", limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n",
      "Retrieved 1000 comments from Pushshift\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "comments_df = pd.DataFrame(comments)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "print(comments_df.columns)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['all_awardings', 'associated_award', 'author',\n",
      "       'author_flair_background_color', 'author_flair_css_class',\n",
      "       'author_flair_richtext', 'author_flair_template_id',\n",
      "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
      "       'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders',\n",
      "       'body', 'collapsed_because_crowd_control', 'collapsed_reason_code',\n",
      "       'comment_type', 'created_utc', 'gildings', 'id', 'is_submitter',\n",
      "       'link_id', 'locked', 'no_follow', 'parent_id', 'permalink',\n",
      "       'retrieved_on', 'score', 'send_replies', 'stickied', 'subreddit',\n",
      "       'subreddit_id', 'top_awarded_type', 'total_awards_received',\n",
      "       'treatment_tags', 'archived', 'body_sha1', 'can_gild', 'collapsed',\n",
      "       'collapsed_reason', 'controversiality', 'distinguished', 'gilded',\n",
      "       'retrieved_utc', 'score_hidden', 'subreddit_name_prefixed',\n",
      "       'subreddit_type', 'media_metadata', 'author_cakeday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}