{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER: https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-rule-based-vader-and-nltk-72067970fb71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_submissions():\n",
    "    data = pd.read_pickle(\"./data/cleaned_submissions.pkl\")\n",
    "    data = data.loc[data[\"sentiment\"] != \"\", [\"text\", \"sentiment\"]] # Only get labeled instances\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vectorize_data(data):\n",
    "    # Encode labels\n",
    "    lenc = LabelEncoder()\n",
    "    y = lenc.fit_transform(data[\"sentiment\"])\n",
    "\n",
    "    # Vectorize text using tfidf\n",
    "    tfidf = TfidfVectorizer(preprocessor=' '.join, lowercase=False, min_df=5) # min_df = Minimum occurance of words\n",
    "    X = tfidf.fit_transform(data[\"text\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_matrices():\n",
    "    X = scipy.sparse.load_npz('./data/X_sparse.npz')\n",
    "    y = np.load(\"./data/y_sparse.npy\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('svc', SVC())]),\n",
       "             param_grid={'svc__C': [3], 'svc__kernel': ['poly']})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        (\"svc\", SVC())\n",
    "    ])\n",
    "\n",
    "param_grid = {\"svc__kernel\": [\"poly\"], \"svc__C\": [3]} #[\"poly\", \"rbf\", \"sigmoid\", \"linear\"], [3, 4, 5, 6, 7]\n",
    "CV = GridSearchCV(pipeline, param_grid, cv = 5)\n",
    "# pipeline.get_params().keys() See all available parameters\n",
    "CV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(CV, open(\"./data/svm_first_model_poly_3.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"./data/svm_first_model_poly_3.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.8837895792141246 can be achieved with the following parameters: {'svc__C': 3, 'svc__kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of {} can be achieved with the following parameters: {}\".format(loaded_model.score(X_test, y_test), CV.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('mnb', MultinomialNB())]),\n",
       "             param_grid={'mnb__alpha': [1]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        (\"mnb\", MultinomialNB())\n",
    "    ])\n",
    "\n",
    "param_grid = {\"mnb__alpha\": [1]}\n",
    "CV = GridSearchCV(pipeline, param_grid, cv = 5)\n",
    "# pipeline.get_params().keys() See all available parameters\n",
    "CV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(CV, open(\"./data/nb_first_model_mnb.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"./data/nb_first_model_mnb.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.7632626918042831 can be achieved with the following parameters: {'mnb__alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of {} can be achieved with the following parameters: {}\".format(loaded_model.score(X_test, y_test), CV.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin DEV #\n",
    "# This DEV actually works!!! Use this!!!\n",
    "# https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_cleaned_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lenc = LabelEncoder()\n",
    "y_train = lenc.fit_transform(data[\"sentiment\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2934\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = 0\n",
    "for sentence in sentences:\n",
    "    if len(sentence) > max_sentence_len:\n",
    "        max_sentence_len = len(sentence)\n",
    "print(max_sentence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.quora.com/What-are-the-strategies-to-deal-with-different-length-of-sentences-for-RNN-or-LSTM\n",
    "1) If the sentences are too long, try to create an embedding that maps the words to a smaller feature space. Take a look at GloVe embeddings, Word2Vec, etc.\n",
    "\n",
    "2) Increase the depth of the RNN. As the sequence length gets longer, it becomes harder and harder for a single layered LSTM to process the dependencies in the data. Adding more hidden layers greatly helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set it lower, because 2934 words in a sentence is quite a lot!!!\n",
    "max_sentence_len = 90 # set it to 100 to have less dimensions for dev purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "for sentence in sentences:\n",
    "    new_sentences.append(sentence[:max_sentence_len])\n",
    "sentences = new_sentences\n",
    "del new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_model = gensim.models.Word2Vec(sentences, vector_size=200, min_count=1, window=5) # Vector_size = number of words??? Check params!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = word_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size =pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  moon -> rocketship (0.98), brrrrr (0.97), rocket (0.97), andromeda (0.97), sticker (0.97), gooooo (0.96), baby (0.96), thinkcheersps (0.96)\n",
      "  short -> 138 (0.94), interest (0.93), cover (0.92), seller (0.92), float (0.92), fridays (0.92), 22642 (0.92), unwind (0.91)\n",
      "  robinhood -> rh (0.98), app (0.98), zealand (0.98), webull (0.98), broker (0.98), allow (0.98), purchase (0.98), uk (0.97)\n",
      "  andromeda -> rocket (0.99), pluto (0.99), mars (0.99), rocketship (0.99), baby (0.98), galaxy (0.98), uranus (0.98), mooooon (0.98)\n",
      "  ape -> fellow (0.98), autist (0.95), retard (0.95), fighting (0.94), grandkid (0.94), stay (0.94), together (0.93), banana (0.93)\n",
      "  ðŸ¦ -> ðŸŒ (0.99), ðŸ¦§ (0.98), ðŸ’ª (0.98), ðŸ™ (0.98), âœ‹ (0.98), ðŸ¤ (0.98), ðŸ¤² (0.98), ðŸ» (0.98)\n"
     ]
    }
   ],
   "source": [
    "for word in ['moon', 'short', 'robinhood', 'andromeda', 'ape', 'ðŸ¦']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                           for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "  print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "  return word_model.wv.key_to_index[word]\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index_to_key[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lstm = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "y_train_lstm = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence):\n",
    "    x_train_lstm[i, t] = word2idx(word)\n",
    "  #y_train_lstm[i] = word2idx(sentence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bullish [0. 1. 0.]\n",
      "2 neutral [0. 0. 1.]\n",
      "0 bearish [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0], data[\"sentiment\"][0], to_categorical(y_train)[0])\n",
    "print(y_train[-2], list(data[\"sentiment\"])[-2], to_categorical(y_train)[-2])\n",
    "print(y_train[6], data[\"sentiment\"][60], to_categorical(y_train)[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "225/225 - 5s - loss: 1.0239 - acc: 0.5100 - val_loss: 1.0158 - val_acc: 0.5358 - 5s/epoch - 21ms/step\n",
      "Epoch 2/20\n",
      "225/225 - 3s - loss: 1.0146 - acc: 0.5177 - val_loss: 1.0285 - val_acc: 0.5355 - 3s/epoch - 15ms/step\n",
      "Epoch 3/20\n",
      "225/225 - 3s - loss: 1.0115 - acc: 0.5198 - val_loss: 1.0485 - val_acc: 0.5336 - 3s/epoch - 15ms/step\n",
      "Epoch 4/20\n",
      "225/225 - 3s - loss: 0.9934 - acc: 0.5333 - val_loss: 1.0403 - val_acc: 0.5341 - 3s/epoch - 15ms/step\n",
      "Epoch 5/20\n",
      "225/225 - 3s - loss: 0.9152 - acc: 0.5654 - val_loss: 0.9727 - val_acc: 0.5338 - 3s/epoch - 15ms/step\n",
      "Epoch 6/20\n",
      "225/225 - 3s - loss: 0.7072 - acc: 0.7018 - val_loss: 0.8926 - val_acc: 0.6352 - 3s/epoch - 15ms/step\n",
      "Epoch 7/20\n",
      "225/225 - 3s - loss: 0.4820 - acc: 0.8058 - val_loss: 0.9591 - val_acc: 0.6332 - 3s/epoch - 15ms/step\n",
      "Epoch 8/20\n",
      "225/225 - 3s - loss: 0.3706 - acc: 0.8570 - val_loss: 1.0357 - val_acc: 0.6140 - 3s/epoch - 15ms/step\n",
      "Epoch 9/20\n",
      "225/225 - 3s - loss: 0.3168 - acc: 0.8798 - val_loss: 1.1400 - val_acc: 0.6280 - 3s/epoch - 15ms/step\n",
      "Epoch 10/20\n",
      "225/225 - 3s - loss: 0.2814 - acc: 0.8926 - val_loss: 1.2818 - val_acc: 0.6188 - 3s/epoch - 15ms/step\n",
      "Epoch 11/20\n",
      "225/225 - 3s - loss: 0.2530 - acc: 0.9043 - val_loss: 1.3109 - val_acc: 0.6321 - 3s/epoch - 15ms/step\n",
      "Epoch 12/20\n",
      "225/225 - 3s - loss: 0.2367 - acc: 0.9114 - val_loss: 1.4743 - val_acc: 0.6163 - 3s/epoch - 15ms/step\n",
      "Epoch 13/20\n",
      "225/225 - 3s - loss: 0.2195 - acc: 0.9175 - val_loss: 1.5692 - val_acc: 0.6118 - 3s/epoch - 15ms/step\n",
      "Epoch 14/20\n",
      "225/225 - 3s - loss: 0.1977 - acc: 0.9286 - val_loss: 1.7784 - val_acc: 0.6021 - 3s/epoch - 15ms/step\n",
      "Epoch 15/20\n",
      "225/225 - 3s - loss: 0.1881 - acc: 0.9314 - val_loss: 1.8077 - val_acc: 0.6076 - 3s/epoch - 15ms/step\n",
      "Epoch 16/20\n",
      "225/225 - 3s - loss: 0.1790 - acc: 0.9348 - val_loss: 1.7202 - val_acc: 0.6009 - 3s/epoch - 15ms/step\n",
      "Epoch 17/20\n",
      "225/225 - 4s - loss: 0.1693 - acc: 0.9384 - val_loss: 1.8564 - val_acc: 0.5996 - 4s/epoch - 16ms/step\n",
      "Epoch 18/20\n",
      "225/225 - 3s - loss: 0.1640 - acc: 0.9385 - val_loss: 2.0418 - val_acc: 0.6090 - 3s/epoch - 15ms/step\n",
      "Epoch 19/20\n",
      "225/225 - 3s - loss: 0.1526 - acc: 0.9432 - val_loss: 2.0273 - val_acc: 0.5976 - 3s/epoch - 15ms/step\n",
      "Epoch 20/20\n",
      "225/225 - 3s - loss: 0.1405 - acc: 0.9490 - val_loss: 2.2719 - val_acc: 0.5959 - 3s/epoch - 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24d874c22e0>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, CuDNNLSTM, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = emdedding_size, weights = [pretrained_weights]))\n",
    "model.add(CuDNNLSTM(units = emdedding_size))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation = \"softmax\"))\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "model.fit(x_train_lstm, y_train, epochs=20, validation_split=0.2, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, None, 200)         4382600   \n",
      "                                                                 \n",
      " cu_dnnlstm_10 (CuDNNLSTM)   (None, 200)               321600    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,704,803\n",
      "Trainable params: 4,704,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End DEV #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_sparse_matrices()\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "Num GPUs Available:  1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten array\n",
    "# Limit Size for DEV\n",
    "X_train_lstm = X_train.toarray()[:10000, :, None]\n",
    "# y_train_lstm = to_categorical(y_train) # To make it 2d\n",
    "# y_train_lstm = y_train_lstm[:10000, :]\n",
    "y_train_lstm = y_train[:10000]\n",
    "# del X_train\n",
    "# del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2], y=[1 1 1 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "classWeight = compute_class_weight('balanced', np.unique(y_train_lstm), y_train_lstm) \n",
    "classWeight = dict(enumerate(classWeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.440214738897023, 1: 0.5340168749332479, 2: 1.3935340022296545}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 213, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_10\" (type Sequential).\n    \n    Input 0 of layer \"cu_dnnlstm_10\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 2050)\n    \n    Call arguments received:\n      â€¢ inputs=<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000206DECB8040>\n      â€¢ training=True\n      â€¢ mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15632/3843339595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassWeight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\DEV\\Master Thesis\\App\\.thesis\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 213, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_10\" (type Sequential).\n    \n    Input 0 of layer \"cu_dnnlstm_10\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 2050)\n    \n    Call arguments received:\n      â€¢ inputs=<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000206DECB8040>\n      â€¢ training=True\n      â€¢ mask=None\n"
     ]
    }
   ],
   "source": [
    "# DEV\n",
    "#from keras.optimizers import adam\n",
    "#opt = SGD(lr=0.01)\n",
    "\n",
    "from keras.layers import Embedding, Dense, CuDNNLSTM, Dropout\n",
    "import tensorflow as tf\n",
    "opti = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "m = Sequential()\n",
    "# m.add(Embedding(X_train_lstm.shape[1], 512)) # Input dim is X_train_lstm.shape[1], dim is the output dimensionality\n",
    "m.add(CuDNNLSTM(32))\n",
    "m.add(Dropout(0.1))\n",
    "m.add(Dense(1, activation = \"softmax\"))\n",
    "m.compile(optimizer = opti, loss =  \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "history = m.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, class_weight = classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, CuDNNLSTM, Dropout\n",
    "\n",
    "def build_model(input_dim, output_dim, hidden_states, opt):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim)) # Input dim is X_train_lstm.shape[1], dim is the output dimensionality\n",
    "    model.add(CuDNNLSTM(hidden_states))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation = \"softmax\"))\n",
    "    model.compile(opt, \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 - 12s - loss: 0.9262 - acc: 0.6177 - val_loss: 0.8930 - val_acc: 0.6345 - 12s/epoch - 98ms/step\n",
      "Epoch 2/5\n",
      "125/125 - 11s - loss: 0.9168 - acc: 0.6216 - val_loss: 0.8927 - val_acc: 0.6345 - 11s/epoch - 92ms/step\n",
      "Epoch 3/5\n",
      "125/125 - 11s - loss: 0.9158 - acc: 0.6216 - val_loss: 0.8937 - val_acc: 0.6345 - 11s/epoch - 91ms/step\n",
      "Epoch 4/5\n",
      "125/125 - 12s - loss: 0.9145 - acc: 0.6216 - val_loss: 0.8932 - val_acc: 0.6345 - 12s/epoch - 92ms/step\n",
      "Epoch 5/5\n",
      "125/125 - 11s - loss: 0.9156 - acc: 0.6216 - val_loss: 0.8928 - val_acc: 0.6345 - 11s/epoch - 91ms/step\n"
     ]
    }
   ],
   "source": [
    "dim = [512] #[256, 512, 1024]\n",
    "hidden_states = [16] #[16, 32, 64]\n",
    "optimi = [\"Adam\"] #[\"rmsprop\", \"SGD\", \"Adam\"]\n",
    "\n",
    "acc = []\n",
    "val_acc = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "param_list = []\n",
    "\n",
    "for d in dim:\n",
    "    for state in hidden_states:\n",
    "        for opt in optimi:\n",
    "            # optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "            # optimizer.learning_rate.assign(0.01)\n",
    "            model = build_model(X_train_lstm.shape[1], d, state, opt)\n",
    "            history = model.fit(X_train_lstm, y_train_lstm, epochs=5, batch_size=64, validation_split=0.2, verbose=2)\n",
    "            acc.append(history.history['acc'])\n",
    "            val_acc.append(history.history['val_acc'])\n",
    "            loss.append(history.history['loss'])\n",
    "            val_loss.append(history.history['val_loss'])\n",
    "            param_list.append(\"Optimizer: \" + opt + \" - States: \" + str(state) + \" - Dimensions:\" + str(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 512)         1049600   \n",
      "                                                                 \n",
      " cu_dnnlstm_3 (CuDNNLSTM)    (None, 32)                69888     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,119,521\n",
      "Trainable params: 1,119,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "from keras.models import load_model\n",
    "history.model.save('./data/small_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('./data/small_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.0, 0.0], 'acc': [0.6518188714981079, 0.6518188714981079]}\n",
      "{'verbose': 0, 'epochs': 2, 'steps': 4489}\n"
     ]
    }
   ],
   "source": [
    "print(history.history) # Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/train_history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_history', 'rb') as file_pi:\n",
    "    h = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.0, 0.0], 'acc': [0.6518188714981079, 0.6518188714981079]}\n"
     ]
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(data_list, label_list, title, xlabel='Epochs', ylabel=None):\n",
    "    ''' Plots a list of vectors.\n",
    "\n",
    "    Parameters:\n",
    "        data_list  : list of vectors containing the values to plot\n",
    "        label_list : list of labels describing the data, one per vector\n",
    "        title      : title of the plot\n",
    "        ylabel     : label for the y axis\n",
    "    '''\n",
    "    epochs = range(1, len(data_list[0]) + 1)\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        plt.plot(epochs, data, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9328/1180918268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m plot_history(data_list=val_loss,\n\u001b[0m\u001b[0;32m      2\u001b[0m              \u001b[0mlabel_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#param_list,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m              \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Comparison of different recurrent layer types'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m              ylabel='Loss')\n\u001b[0;32m      5\u001b[0m plot_history(data_list=val_acc,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(data_list=val_loss,\n",
    "             label_list=[[\"\"]*len(param_list)], #param_list,\n",
    "             title='Comparison of different recurrent layer types',\n",
    "             ylabel='Loss')\n",
    "plot_history(data_list=val_acc,\n",
    "             label_list=[[\"\"]*len(param_list)], #param_list,\n",
    "             title='Comparison of different recurrent layer types',\n",
    "             ylabel='Validation accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a199850fbe9b2f1658a16eea735881451fc009eba53b7dda86327ce82228d5dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.thesis': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
